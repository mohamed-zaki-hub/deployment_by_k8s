
==> Audit <==
|---------|--------------------------------|----------|-------|---------|----------------------|----------------------|
| Command |              Args              | Profile  | User  | Version |      Start Time      |       End Time       |
|---------|--------------------------------|----------|-------|---------|----------------------|----------------------|
| start   |                                | minikube | adawy | v1.34.0 | 12 Oct 24 15:34 EEST | 12 Oct 24 15:47 EEST |
| start   |                                | minikube | adawy | v1.34.0 | 18 Oct 24 21:09 EEST | 18 Oct 24 21:10 EEST |
| service | -n mongo-app-service --url     | minikube | adawy | v1.34.0 | 18 Oct 24 21:22 EEST |                      |
| service | -n go-depi mongo-app-service   | minikube | adawy | v1.34.0 | 18 Oct 24 21:22 EEST |                      |
|         | --url                          |          |       |         |                      |                      |
| service | -n go-depi mongo-app-service   | minikube | adawy | v1.34.0 | 18 Oct 24 21:24 EEST |                      |
|         | --url                          |          |       |         |                      |                      |
| service | -n go-depi mongo-app-service   | minikube | adawy | v1.34.0 | 18 Oct 24 21:29 EEST |                      |
|         | --url                          |          |       |         |                      |                      |
| service | -n go-depi mongo-app-service   | minikube | adawy | v1.34.0 | 18 Oct 24 21:35 EEST |                      |
|         | --url                          |          |       |         |                      |                      |
| service | -n go-depi mongo-app-service   | minikube | adawy | v1.34.0 | 18 Oct 24 21:39 EEST |                      |
|         | --url                          |          |       |         |                      |                      |
|---------|--------------------------------|----------|-------|---------|----------------------|----------------------|


==> Last Start <==
Log file created at: 2024/10/18 21:09:02
Running on machine: adawy-Lenovo-ideapad-500-15ISK
Binary: Built with gc go1.22.5 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1018 21:09:02.781132   42144 out.go:345] Setting OutFile to fd 1 ...
I1018 21:09:02.782421   42144 out.go:397] isatty.IsTerminal(1) = true
I1018 21:09:02.782434   42144 out.go:358] Setting ErrFile to fd 2...
I1018 21:09:02.782450   42144 out.go:397] isatty.IsTerminal(2) = true
I1018 21:09:02.783412   42144 root.go:338] Updating PATH: /home/adawy/.minikube/bin
W1018 21:09:02.783823   42144 root.go:314] Error reading config file at /home/adawy/.minikube/config/config.json: open /home/adawy/.minikube/config/config.json: no such file or directory
I1018 21:09:02.799401   42144 out.go:352] Setting JSON to false
I1018 21:09:02.802778   42144 start.go:129] hostinfo: {"hostname":"adawy-Lenovo-ideapad-500-15ISK","uptime":92704,"bootTime":1729182239,"procs":292,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"6.8.0-45-generic","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"29464012-cc71-4b82-9c67-bfbb9c07221e"}
I1018 21:09:02.802878   42144 start.go:139] virtualization:  
I1018 21:09:02.814015   42144 out.go:177] üòÑ  minikube v1.34.0 on Ubuntu 22.04
I1018 21:09:02.865543   42144 notify.go:220] Checking for updates...
I1018 21:09:02.867242   42144 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1018 21:09:02.867568   42144 driver.go:394] Setting default libvirt URI to qemu:///system
I1018 21:09:02.994523   42144 docker.go:123] docker version: linux-27.3.1:Docker Engine - Community
I1018 21:09:02.995047   42144 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1018 21:09:05.258357   42144 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (2.263202832s)
I1018 21:09:05.260161   42144 info.go:266] docker info: {ID:1c37f0c2-4f5f-4e35-b98e-6d14f9b8d104 Containers:10 ContainersRunning:1 ContainersPaused:0 ContainersStopped:9 Images:8 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:30 OomKillDisable:false NGoroutines:49 SystemTime:2024-10-18 21:09:05.21074893 +0300 EEST LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.8.0-45-generic OperatingSystem:Ubuntu 22.04.5 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8239865856 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:adawy-Lenovo-ideapad-500-15ISK Labels:[] ExperimentalBuild:false ServerVersion:27.3.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:7f7fdf5fed64eb6a7caf99b3e12efcf9d60e311c Expected:7f7fdf5fed64eb6a7caf99b3e12efcf9d60e311c} RuncCommit:{ID:v1.1.14-0-g2c9f560 Expected:v1.1.14-0-g2c9f560} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:[WARNING: bridge-nf-call-iptables is disabled WARNING: bridge-nf-call-ip6tables is disabled] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.17.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.29.7]] Warnings:<nil>}}
I1018 21:09:05.260483   42144 docker.go:318] overlay module found
I1018 21:09:05.267785   42144 out.go:177] ‚ú®  Using the docker driver based on existing profile
I1018 21:09:05.271424   42144 start.go:297] selected driver: docker
I1018 21:09:05.271447   42144 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/adawy:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1018 21:09:05.271778   42144 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1018 21:09:05.272035   42144 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1018 21:09:05.391068   42144 info.go:266] docker info: {ID:1c37f0c2-4f5f-4e35-b98e-6d14f9b8d104 Containers:10 ContainersRunning:1 ContainersPaused:0 ContainersStopped:9 Images:8 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:30 OomKillDisable:false NGoroutines:49 SystemTime:2024-10-18 21:09:05.351433628 +0300 EEST LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.8.0-45-generic OperatingSystem:Ubuntu 22.04.5 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8239865856 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:adawy-Lenovo-ideapad-500-15ISK Labels:[] ExperimentalBuild:false ServerVersion:27.3.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:7f7fdf5fed64eb6a7caf99b3e12efcf9d60e311c Expected:7f7fdf5fed64eb6a7caf99b3e12efcf9d60e311c} RuncCommit:{ID:v1.1.14-0-g2c9f560 Expected:v1.1.14-0-g2c9f560} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:[WARNING: bridge-nf-call-iptables is disabled WARNING: bridge-nf-call-ip6tables is disabled] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.17.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.29.7]] Warnings:<nil>}}
I1018 21:09:05.391855   42144 cni.go:84] Creating CNI manager for ""
I1018 21:09:05.391872   42144 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1018 21:09:05.391929   42144 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/adawy:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1018 21:09:05.397025   42144 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I1018 21:09:05.400389   42144 cache.go:121] Beginning downloading kic base image for docker with docker
I1018 21:09:05.406877   42144 out.go:177] üöú  Pulling base image v0.0.45 ...
I1018 21:09:05.411805   42144 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1018 21:09:05.412060   42144 preload.go:146] Found local preload: /home/adawy/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4
I1018 21:09:05.412080   42144 cache.go:56] Caching tarball of preloaded images
I1018 21:09:05.412093   42144 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local docker daemon
I1018 21:09:05.412440   42144 preload.go:172] Found /home/adawy/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1018 21:09:05.412482   42144 cache.go:59] Finished verifying existence of preloaded tar for v1.31.0 on docker
I1018 21:09:05.412814   42144 profile.go:143] Saving config to /home/adawy/.minikube/profiles/minikube/config.json ...
W1018 21:09:05.523400   42144 image.go:95] image gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 is of wrong architecture
I1018 21:09:05.523420   42144 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 to local cache
I1018 21:09:05.523813   42144 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory
I1018 21:09:05.538258   42144 image.go:66] Found gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory, skipping pull
I1018 21:09:05.538291   42144 image.go:135] gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 exists in cache, skipping pull
I1018 21:09:05.538339   42144 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 as a tarball
I1018 21:09:05.538362   42144 cache.go:162] Loading gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from local cache
I1018 21:09:07.449404   42144 cache.go:164] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from cached tarball
I1018 21:09:07.449521   42144 cache.go:194] Successfully downloaded all kic artifacts
I1018 21:09:07.449651   42144 start.go:360] acquireMachinesLock for minikube: {Name:mk8a21b46f493b4b821631d0acc3af61752a267b Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1018 21:09:07.449935   42144 start.go:364] duration metric: took 225.729¬µs to acquireMachinesLock for "minikube"
I1018 21:09:07.449991   42144 start.go:96] Skipping create...Using existing machine configuration
I1018 21:09:07.450025   42144 fix.go:54] fixHost starting: 
I1018 21:09:07.451573   42144 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1018 21:09:07.543298   42144 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1018 21:09:07.543384   42144 fix.go:138] unexpected machine state, will restart: <nil>
I1018 21:09:07.551789   42144 out.go:177] üîÑ  Restarting existing docker container for "minikube" ...
I1018 21:09:07.555660   42144 cli_runner.go:164] Run: docker start minikube
I1018 21:09:09.246356   42144 cli_runner.go:217] Completed: docker start minikube: (1.690605896s)
I1018 21:09:09.246496   42144 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1018 21:09:09.294459   42144 kic.go:430] container "minikube" state is running.
I1018 21:09:09.294954   42144 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1018 21:09:09.317561   42144 profile.go:143] Saving config to /home/adawy/.minikube/profiles/minikube/config.json ...
I1018 21:09:09.317816   42144 machine.go:93] provisionDockerMachine start ...
I1018 21:09:09.317876   42144 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1018 21:09:09.344962   42144 main.go:141] libmachine: Using SSH client type: native
I1018 21:09:09.351734   42144 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1018 21:09:09.351746   42144 main.go:141] libmachine: About to run SSH command:
hostname
I1018 21:09:09.352425   42144 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:59398->127.0.0.1:32768: read: connection reset by peer
I1018 21:09:12.355180   42144 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:41388->127.0.0.1:32768: read: connection reset by peer
I1018 21:09:15.921201   42144 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1018 21:09:15.921266   42144 ubuntu.go:169] provisioning hostname "minikube"
I1018 21:09:15.921546   42144 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1018 21:09:15.983465   42144 main.go:141] libmachine: Using SSH client type: native
I1018 21:09:15.983838   42144 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1018 21:09:15.983852   42144 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1018 21:09:16.929598   42144 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1018 21:09:16.929901   42144 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1018 21:09:16.987077   42144 main.go:141] libmachine: Using SSH client type: native
I1018 21:09:16.987356   42144 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1018 21:09:16.987399   42144 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1018 21:09:17.141054   42144 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1018 21:09:17.141143   42144 ubuntu.go:175] set auth options {CertDir:/home/adawy/.minikube CaCertPath:/home/adawy/.minikube/certs/ca.pem CaPrivateKeyPath:/home/adawy/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/adawy/.minikube/machines/server.pem ServerKeyPath:/home/adawy/.minikube/machines/server-key.pem ClientKeyPath:/home/adawy/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/adawy/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/adawy/.minikube}
I1018 21:09:17.141236   42144 ubuntu.go:177] setting up certificates
I1018 21:09:17.141269   42144 provision.go:84] configureAuth start
I1018 21:09:17.141442   42144 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1018 21:09:17.197419   42144 provision.go:143] copyHostCerts
I1018 21:09:17.204251   42144 exec_runner.go:144] found /home/adawy/.minikube/ca.pem, removing ...
I1018 21:09:17.204262   42144 exec_runner.go:203] rm: /home/adawy/.minikube/ca.pem
I1018 21:09:17.204321   42144 exec_runner.go:151] cp: /home/adawy/.minikube/certs/ca.pem --> /home/adawy/.minikube/ca.pem (1074 bytes)
I1018 21:09:17.229396   42144 exec_runner.go:144] found /home/adawy/.minikube/cert.pem, removing ...
I1018 21:09:17.229441   42144 exec_runner.go:203] rm: /home/adawy/.minikube/cert.pem
I1018 21:09:17.229679   42144 exec_runner.go:151] cp: /home/adawy/.minikube/certs/cert.pem --> /home/adawy/.minikube/cert.pem (1119 bytes)
I1018 21:09:17.240490   42144 exec_runner.go:144] found /home/adawy/.minikube/key.pem, removing ...
I1018 21:09:17.240514   42144 exec_runner.go:203] rm: /home/adawy/.minikube/key.pem
I1018 21:09:17.240687   42144 exec_runner.go:151] cp: /home/adawy/.minikube/certs/key.pem --> /home/adawy/.minikube/key.pem (1675 bytes)
I1018 21:09:17.274001   42144 provision.go:117] generating server cert: /home/adawy/.minikube/machines/server.pem ca-key=/home/adawy/.minikube/certs/ca.pem private-key=/home/adawy/.minikube/certs/ca-key.pem org=adawy.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I1018 21:09:17.532324   42144 provision.go:177] copyRemoteCerts
I1018 21:09:17.532431   42144 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1018 21:09:17.532469   42144 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1018 21:09:17.554142   42144 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/adawy/.minikube/machines/minikube/id_rsa Username:docker}
I1018 21:09:17.664498   42144 ssh_runner.go:362] scp /home/adawy/.minikube/machines/server.pem --> /etc/docker/server.pem (1176 bytes)
I1018 21:09:18.049254   42144 ssh_runner.go:362] scp /home/adawy/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1018 21:09:18.341429   42144 ssh_runner.go:362] scp /home/adawy/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I1018 21:09:18.423895   42144 provision.go:87] duration metric: took 1.282605364s to configureAuth
I1018 21:09:18.423922   42144 ubuntu.go:193] setting minikube options for container-runtime
I1018 21:09:18.424390   42144 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1018 21:09:18.424501   42144 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1018 21:09:18.449033   42144 main.go:141] libmachine: Using SSH client type: native
I1018 21:09:18.449241   42144 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1018 21:09:18.449260   42144 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1018 21:09:18.690832   42144 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1018 21:09:18.690867   42144 ubuntu.go:71] root file system type: overlay
I1018 21:09:18.691425   42144 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1018 21:09:18.691756   42144 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1018 21:09:18.744367   42144 main.go:141] libmachine: Using SSH client type: native
I1018 21:09:18.744569   42144 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1018 21:09:18.744677   42144 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1018 21:09:18.961583   42144 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1018 21:09:18.961739   42144 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1018 21:09:18.988837   42144 main.go:141] libmachine: Using SSH client type: native
I1018 21:09:18.989067   42144 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1018 21:09:18.989101   42144 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1018 21:09:19.371347   42144 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1018 21:09:19.371389   42144 machine.go:96] duration metric: took 10.053550085s to provisionDockerMachine
I1018 21:09:19.371437   42144 start.go:293] postStartSetup for "minikube" (driver="docker")
I1018 21:09:19.371511   42144 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1018 21:09:19.371772   42144 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1018 21:09:19.371964   42144 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1018 21:09:19.431418   42144 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/adawy/.minikube/machines/minikube/id_rsa Username:docker}
I1018 21:09:19.593390   42144 ssh_runner.go:195] Run: cat /etc/os-release
I1018 21:09:19.604411   42144 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1018 21:09:19.604470   42144 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1018 21:09:19.604495   42144 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1018 21:09:19.604507   42144 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I1018 21:09:19.604524   42144 filesync.go:126] Scanning /home/adawy/.minikube/addons for local assets ...
I1018 21:09:19.605057   42144 filesync.go:126] Scanning /home/adawy/.minikube/files for local assets ...
I1018 21:09:19.605414   42144 start.go:296] duration metric: took 233.951912ms for postStartSetup
I1018 21:09:19.605494   42144 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1018 21:09:19.605647   42144 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1018 21:09:19.636429   42144 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/adawy/.minikube/machines/minikube/id_rsa Username:docker}
I1018 21:09:19.801328   42144 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1018 21:09:19.820685   42144 fix.go:56] duration metric: took 12.370640822s for fixHost
I1018 21:09:19.820720   42144 start.go:83] releasing machines lock for "minikube", held for 12.370758489s
I1018 21:09:19.820948   42144 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1018 21:09:19.898128   42144 ssh_runner.go:195] Run: cat /version.json
I1018 21:09:19.898290   42144 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1018 21:09:19.898840   42144 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1018 21:09:19.899059   42144 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1018 21:09:19.964102   42144 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/adawy/.minikube/machines/minikube/id_rsa Username:docker}
I1018 21:09:19.964720   42144 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/adawy/.minikube/machines/minikube/id_rsa Username:docker}
I1018 21:09:20.149081   42144 ssh_runner.go:195] Run: systemctl --version
I1018 21:09:22.697869   42144 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (2.798966858s)
I1018 21:09:22.698263   42144 ssh_runner.go:235] Completed: systemctl --version: (2.549132019s)
I1018 21:09:22.698474   42144 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1018 21:09:22.974486   42144 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I1018 21:09:23.075680   42144 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I1018 21:09:23.075867   42144 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1018 21:09:23.104261   42144 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I1018 21:09:23.104290   42144 start.go:495] detecting cgroup driver to use...
I1018 21:09:23.104333   42144 detect.go:190] detected "systemd" cgroup driver on host os
I1018 21:09:23.104505   42144 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1018 21:09:23.130982   42144 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I1018 21:09:23.146087   42144 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1018 21:09:23.160931   42144 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I1018 21:09:23.160986   42144 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I1018 21:09:23.174896   42144 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1018 21:09:23.187939   42144 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1018 21:09:23.201175   42144 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1018 21:09:23.214183   42144 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1018 21:09:23.233944   42144 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1018 21:09:23.247216   42144 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1018 21:09:23.260720   42144 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1018 21:09:23.274034   42144 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1018 21:09:23.293423   42144 crio.go:166] couldn't verify netfilter by "sudo sysctl net.bridge.bridge-nf-call-iptables" which might be okay. error: sudo sysctl net.bridge.bridge-nf-call-iptables: Process exited with status 255
stdout:

stderr:
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directory
I1018 21:09:23.293479   42144 ssh_runner.go:195] Run: sudo modprobe br_netfilter
I1018 21:09:23.469277   42144 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1018 21:09:23.500997   42144 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1018 21:09:23.666012   42144 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1018 21:09:23.763086   42144 start.go:495] detecting cgroup driver to use...
I1018 21:09:23.763118   42144 detect.go:190] detected "systemd" cgroup driver on host os
I1018 21:09:23.763168   42144 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1018 21:09:23.779047   42144 cruntime.go:279] skipping containerd shutdown because we are bound to it
I1018 21:09:23.779120   42144 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1018 21:09:23.882531   42144 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1018 21:09:23.936929   42144 ssh_runner.go:195] Run: which cri-dockerd
I1018 21:09:23.944262   42144 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1018 21:09:23.961531   42144 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I1018 21:09:23.985714   42144 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1018 21:09:24.151671   42144 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1018 21:09:24.251858   42144 docker.go:574] configuring docker to use "systemd" as cgroup driver...
I1018 21:09:24.252005   42144 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I1018 21:09:24.327586   42144 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1018 21:09:24.435017   42144 ssh_runner.go:195] Run: sudo systemctl restart docker
I1018 21:09:28.189438   42144 ssh_runner.go:235] Completed: sudo systemctl restart docker: (3.754377536s)
I1018 21:09:28.189505   42144 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1018 21:09:28.208464   42144 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I1018 21:09:28.226965   42144 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1018 21:09:28.241103   42144 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1018 21:09:28.368275   42144 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1018 21:09:28.466522   42144 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1018 21:09:28.596481   42144 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1018 21:09:28.626713   42144 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1018 21:09:28.641455   42144 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1018 21:09:28.735536   42144 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1018 21:09:31.889118   42144 ssh_runner.go:235] Completed: sudo systemctl restart cri-docker.service: (3.153505927s)
I1018 21:09:31.889195   42144 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1018 21:09:31.889522   42144 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1018 21:09:31.917427   42144 start.go:563] Will wait 60s for crictl version
I1018 21:09:31.917594   42144 ssh_runner.go:195] Run: which crictl
I1018 21:09:31.934975   42144 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1018 21:09:33.177051   42144 ssh_runner.go:235] Completed: sudo /usr/bin/crictl version: (1.242014794s)
I1018 21:09:33.177097   42144 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.2.0
RuntimeApiVersion:  v1
I1018 21:09:33.177265   42144 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1018 21:09:34.111173   42144 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1018 21:09:34.155034   42144 out.go:235] üê≥  Preparing Kubernetes v1.31.0 on Docker 27.2.0 ...
I1018 21:09:34.155130   42144 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1018 21:09:34.184709   42144 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I1018 21:09:34.189660   42144 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1018 21:09:34.205467   42144 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/adawy:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I1018 21:09:34.205581   42144 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1018 21:09:34.205680   42144 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1018 21:09:34.248515   42144 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1018 21:09:34.248528   42144 docker.go:615] Images already preloaded, skipping extraction
I1018 21:09:34.248658   42144 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1018 21:09:34.273725   42144 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1018 21:09:34.273741   42144 cache_images.go:84] Images are preloaded, skipping loading
I1018 21:09:34.273753   42144 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.31.0 docker true true} ...
I1018 21:09:34.273890   42144 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.31.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1018 21:09:34.273944   42144 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1018 21:09:36.244755   42144 ssh_runner.go:235] Completed: docker info --format {{.CgroupDriver}}: (1.970761734s)
I1018 21:09:36.245234   42144 cni.go:84] Creating CNI manager for ""
I1018 21:09:36.245298   42144 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1018 21:09:36.245344   42144 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I1018 21:09:36.245395   42144 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.31.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1018 21:09:36.245840   42144 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.31.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1018 21:09:36.245987   42144 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.31.0
I1018 21:09:36.317702   42144 binaries.go:44] Found k8s binaries, skipping transfer
I1018 21:09:36.317905   42144 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1018 21:09:36.353519   42144 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I1018 21:09:36.389031   42144 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1018 21:09:36.417714   42144 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2149 bytes)
I1018 21:09:36.468906   42144 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1018 21:09:36.473589   42144 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1018 21:09:36.489482   42144 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1018 21:09:36.592535   42144 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1018 21:09:36.623865   42144 certs.go:68] Setting up /home/adawy/.minikube/profiles/minikube for IP: 192.168.49.2
I1018 21:09:36.623874   42144 certs.go:194] generating shared ca certs ...
I1018 21:09:36.623888   42144 certs.go:226] acquiring lock for ca certs: {Name:mka81bfb865db169976b39a36a87c6889b0ef294 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1018 21:09:36.624143   42144 certs.go:235] skipping valid "minikubeCA" ca cert: /home/adawy/.minikube/ca.key
I1018 21:09:36.630591   42144 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/adawy/.minikube/proxy-client-ca.key
I1018 21:09:36.630659   42144 certs.go:256] generating profile certs ...
I1018 21:09:36.631007   42144 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/adawy/.minikube/profiles/minikube/client.key
I1018 21:09:36.661697   42144 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /home/adawy/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I1018 21:09:36.662496   42144 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/adawy/.minikube/profiles/minikube/proxy-client.key
I1018 21:09:36.663336   42144 certs.go:484] found cert: /home/adawy/.minikube/certs/ca-key.pem (1679 bytes)
I1018 21:09:36.663513   42144 certs.go:484] found cert: /home/adawy/.minikube/certs/ca.pem (1074 bytes)
I1018 21:09:36.663703   42144 certs.go:484] found cert: /home/adawy/.minikube/certs/cert.pem (1119 bytes)
I1018 21:09:36.663855   42144 certs.go:484] found cert: /home/adawy/.minikube/certs/key.pem (1675 bytes)
I1018 21:09:36.681394   42144 ssh_runner.go:362] scp /home/adawy/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1018 21:09:36.800460   42144 ssh_runner.go:362] scp /home/adawy/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1018 21:09:36.847924   42144 ssh_runner.go:362] scp /home/adawy/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1018 21:09:36.881122   42144 ssh_runner.go:362] scp /home/adawy/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I1018 21:09:36.915007   42144 ssh_runner.go:362] scp /home/adawy/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I1018 21:09:36.950539   42144 ssh_runner.go:362] scp /home/adawy/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1018 21:09:36.983652   42144 ssh_runner.go:362] scp /home/adawy/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1018 21:09:37.018814   42144 ssh_runner.go:362] scp /home/adawy/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I1018 21:09:37.055149   42144 ssh_runner.go:362] scp /home/adawy/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1018 21:09:37.129324   42144 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (740 bytes)
I1018 21:09:37.170982   42144 ssh_runner.go:195] Run: openssl version
I1018 21:09:37.409125   42144 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1018 21:09:37.525933   42144 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1018 21:09:37.551324   42144 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Oct 12 12:46 /usr/share/ca-certificates/minikubeCA.pem
I1018 21:09:37.551651   42144 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1018 21:09:37.602150   42144 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1018 21:09:37.637693   42144 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1018 21:09:37.645557   42144 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I1018 21:09:37.674524   42144 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I1018 21:09:37.717761   42144 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I1018 21:09:37.762248   42144 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I1018 21:09:37.823009   42144 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I1018 21:09:37.869691   42144 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I1018 21:09:37.893815   42144 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/adawy:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1018 21:09:37.894160   42144 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1018 21:09:37.959570   42144 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1018 21:09:37.986109   42144 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I1018 21:09:37.986125   42144 kubeadm.go:593] restartPrimaryControlPlane start ...
I1018 21:09:37.986209   42144 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1018 21:09:38.073246   42144 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1018 21:09:38.080640   42144 kubeconfig.go:125] found "minikube" server: "https://192.168.49.2:8443"
I1018 21:09:38.760953   42144 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1018 21:09:38.825207   42144 kubeadm.go:630] The running cluster does not require reconfiguration: 192.168.49.2
I1018 21:09:38.825258   42144 kubeadm.go:597] duration metric: took 839.11859ms to restartPrimaryControlPlane
I1018 21:09:38.825285   42144 kubeadm.go:394] duration metric: took 931.547785ms to StartCluster
I1018 21:09:38.825329   42144 settings.go:142] acquiring lock: {Name:mk2c13595b4c436065af6c68e47e2e727c48c008 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1018 21:09:38.825582   42144 settings.go:150] Updating kubeconfig:  /home/adawy/.kube/config
I1018 21:09:38.827728   42144 lock.go:35] WriteFile acquiring /home/adawy/.kube/config: {Name:mkc986e75247e57f8e7d2d335989f06f6dc5eef2 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1018 21:09:38.828375   42144 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1018 21:09:38.828523   42144 addons.go:507] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I1018 21:09:38.828820   42144 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1018 21:09:38.828991   42144 addons.go:234] Setting addon storage-provisioner=true in "minikube"
W1018 21:09:38.829023   42144 addons.go:243] addon storage-provisioner should already be in state true
I1018 21:09:38.829020   42144 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1018 21:09:38.829111   42144 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1018 21:09:38.829115   42144 host.go:66] Checking if "minikube" exists ...
I1018 21:09:38.829222   42144 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1018 21:09:38.830721   42144 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1018 21:09:38.831872   42144 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1018 21:09:38.847477   42144 out.go:177] üîé  Verifying Kubernetes components...
I1018 21:09:38.867396   42144 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1018 21:09:38.922458   42144 addons.go:234] Setting addon default-storageclass=true in "minikube"
W1018 21:09:38.922472   42144 addons.go:243] addon default-storageclass should already be in state true
I1018 21:09:38.922506   42144 host.go:66] Checking if "minikube" exists ...
I1018 21:09:38.923404   42144 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1018 21:09:38.957711   42144 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1018 21:09:38.993671   42144 addons.go:431] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1018 21:09:38.993778   42144 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1018 21:09:38.994108   42144 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1018 21:09:39.004576   42144 addons.go:431] installing /etc/kubernetes/addons/storageclass.yaml
I1018 21:09:39.004619   42144 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1018 21:09:39.004751   42144 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1018 21:09:39.042420   42144 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/adawy/.minikube/machines/minikube/id_rsa Username:docker}
I1018 21:09:39.047492   42144 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/adawy/.minikube/machines/minikube/id_rsa Username:docker}
I1018 21:09:39.098546   42144 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1018 21:09:39.115675   42144 api_server.go:52] waiting for apiserver process to appear ...
I1018 21:09:39.115756   42144 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1018 21:09:39.244096   42144 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1018 21:09:39.244096   42144 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1018 21:09:39.616741   42144 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1018 21:09:46.705558   42144 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (7.461434434s)
W1018 21:09:46.705589   42144 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1018 21:09:46.705643   42144 retry.go:31] will retry after 196.857034ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1018 21:09:46.706155   42144 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (7.462040034s)
W1018 21:09:46.706184   42144 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1018 21:09:46.706198   42144 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (7.08942625s)
I1018 21:09:46.706197   42144 retry.go:31] will retry after 328.833323ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1018 21:09:46.706266   42144 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1018 21:09:46.903398   42144 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W1018 21:09:47.031515   42144 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1018 21:09:47.031532   42144 retry.go:31] will retry after 394.05732ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1018 21:09:47.035792   42144 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W1018 21:09:47.114551   42144 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1018 21:09:47.114573   42144 retry.go:31] will retry after 257.60166ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1018 21:09:47.116741   42144 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1018 21:09:47.373674   42144 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1018 21:09:47.426002   42144 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W1018 21:09:47.522154   42144 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1018 21:09:47.522189   42144 retry.go:31] will retry after 334.834382ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W1018 21:09:47.542082   42144 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1018 21:09:47.542100   42144 retry.go:31] will retry after 498.143135ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1018 21:09:47.616153   42144 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1018 21:09:47.857772   42144 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1018 21:09:48.040843   42144 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W1018 21:09:48.045174   42144 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1018 21:09:48.045194   42144 retry.go:31] will retry after 523.21671ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1018 21:09:48.115985   42144 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1018 21:09:48.126707   42144 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1018 21:09:48.126728   42144 retry.go:31] will retry after 572.379183ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1018 21:09:48.568719   42144 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1018 21:09:48.615966   42144 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1018 21:09:48.699902   42144 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W1018 21:09:48.734047   42144 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1018 21:09:48.734071   42144 retry.go:31] will retry after 875.953213ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W1018 21:09:48.787486   42144 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1018 21:09:48.787503   42144 retry.go:31] will retry after 1.065912038s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1018 21:09:49.116066   42144 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1018 21:09:49.610988   42144 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1018 21:09:49.616641   42144 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1018 21:09:49.852498   42144 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1018 21:09:49.852529   42144 retry.go:31] will retry after 2.170978548s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1018 21:09:49.853612   42144 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I1018 21:09:50.116214   42144 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1018 21:09:50.205799   42144 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1018 21:09:50.205817   42144 retry.go:31] will retry after 2.15898224s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1018 21:09:50.616747   42144 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1018 21:09:51.116892   42144 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1018 21:09:51.615943   42144 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1018 21:09:51.681396   42144 api_server.go:72] duration metric: took 12.852928682s to wait for apiserver process to appear ...
I1018 21:09:51.681430   42144 api_server.go:88] waiting for apiserver healthz status ...
I1018 21:09:51.681483   42144 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1018 21:09:51.682498   42144 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1018 21:09:52.024280   42144 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1018 21:09:52.181757   42144 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1018 21:09:52.182157   42144 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
W1018 21:09:52.188374   42144 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1018 21:09:52.188391   42144 retry.go:31] will retry after 3.818095152s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1018 21:09:52.365824   42144 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W1018 21:09:52.556940   42144 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1018 21:09:52.556975   42144 retry.go:31] will retry after 3.275944831s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1018 21:09:52.682317   42144 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1018 21:09:52.683362   42144 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1018 21:09:53.182563   42144 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1018 21:09:53.183544   42144 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1018 21:09:53.681719   42144 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1018 21:09:53.683113   42144 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1018 21:09:54.181765   42144 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1018 21:09:54.183149   42144 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1018 21:09:54.681707   42144 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1018 21:09:54.682868   42144 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1018 21:09:55.182408   42144 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1018 21:09:55.183255   42144 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1018 21:09:55.681692   42144 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1018 21:09:55.682068   42144 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1018 21:09:55.833329   42144 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W1018 21:09:55.901762   42144 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1018 21:09:55.901779   42144 retry.go:31] will retry after 3.734629267s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1018 21:09:56.007036   42144 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W1018 21:09:56.086382   42144 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1018 21:09:56.086404   42144 retry.go:31] will retry after 3.371137569s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1018 21:09:56.182462   42144 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1018 21:09:56.182887   42144 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1018 21:09:56.682573   42144 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1018 21:09:56.683502   42144 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1018 21:09:57.181993   42144 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1018 21:09:57.182995   42144 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1018 21:09:57.681651   42144 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1018 21:09:57.682519   42144 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1018 21:09:58.181710   42144 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1018 21:09:58.182891   42144 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1018 21:09:58.682513   42144 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1018 21:09:58.683557   42144 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1018 21:09:59.181772   42144 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1018 21:09:59.182671   42144 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1018 21:09:59.458218   42144 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W1018 21:09:59.561997   42144 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1018 21:09:59.562014   42144 retry.go:31] will retry after 3.943019374s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1018 21:09:59.637636   42144 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I1018 21:09:59.681938   42144 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1018 21:09:59.682485   42144 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
W1018 21:09:59.746140   42144 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1018 21:09:59.746161   42144 retry.go:31] will retry after 6.950203157s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1018 21:10:00.181648   42144 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1018 21:10:00.182510   42144 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1018 21:10:00.681779   42144 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1018 21:10:00.682767   42144 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1018 21:10:01.181959   42144 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1018 21:10:01.183095   42144 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1018 21:10:01.681587   42144 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1018 21:10:01.683070   42144 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1018 21:10:02.181666   42144 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1018 21:10:02.182786   42144 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1018 21:10:02.682259   42144 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1018 21:10:02.682565   42144 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1018 21:10:03.181494   42144 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1018 21:10:03.181918   42144 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1018 21:10:03.505518   42144 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W1018 21:10:03.611715   42144 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1018 21:10:03.611780   42144 retry.go:31] will retry after 13.142570964s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1018 21:10:03.681905   42144 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1018 21:10:03.682845   42144 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1018 21:10:04.182690   42144 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1018 21:10:04.183938   42144 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1018 21:10:04.682509   42144 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1018 21:10:04.683290   42144 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1018 21:10:05.181787   42144 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1018 21:10:05.183234   42144 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1018 21:10:05.681687   42144 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1018 21:10:05.682479   42144 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1018 21:10:06.181647   42144 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1018 21:10:06.697096   42144 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I1018 21:10:09.928458   42144 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1018 21:10:09.928476   42144 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1018 21:10:09.928487   42144 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1018 21:10:09.961517   42144 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1018 21:10:09.961536   42144 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1018 21:10:10.181854   42144 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1018 21:10:10.194889   42144 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1018 21:10:10.194937   42144 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1018 21:10:10.682563   42144 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1018 21:10:10.695738   42144 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1018 21:10:10.695774   42144 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1018 21:10:11.182439   42144 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1018 21:10:11.188062   42144 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I1018 21:10:11.194349   42144 api_server.go:141] control plane version: v1.31.0
I1018 21:10:11.194364   42144 api_server.go:131] duration metric: took 19.51292488s to wait for apiserver health ...
I1018 21:10:11.194372   42144 system_pods.go:43] waiting for kube-system pods to appear ...
I1018 21:10:11.533922   42144 system_pods.go:59] 7 kube-system pods found
I1018 21:10:11.533974   42144 system_pods.go:61] "coredns-6f6b679f8f-dk69f" [30a9beb3-8b80-4f75-920f-a778311913bb] Running
I1018 21:10:11.533996   42144 system_pods.go:61] "etcd-minikube" [4bf3e5a4-0798-4579-9381-12012076f970] Running
I1018 21:10:11.534014   42144 system_pods.go:61] "kube-apiserver-minikube" [b0faa47d-2ba8-4c21-ab30-1e34ab89c93e] Running
I1018 21:10:11.534033   42144 system_pods.go:61] "kube-controller-manager-minikube" [0bb7f9d6-808d-4d86-940f-b1db55745afb] Running
I1018 21:10:11.534050   42144 system_pods.go:61] "kube-proxy-wbwdt" [1d4bc787-5cfb-461f-bc2d-79e8079e30c0] Running
I1018 21:10:11.534064   42144 system_pods.go:61] "kube-scheduler-minikube" [e0554ee4-98fc-46ea-9566-880d8e88d74f] Running
I1018 21:10:11.534078   42144 system_pods.go:61] "storage-provisioner" [0e6baee1-f7cc-488e-bae2-3755b388334d] Running
I1018 21:10:11.534096   42144 system_pods.go:74] duration metric: took 339.708858ms to wait for pod list to return data ...
I1018 21:10:11.534130   42144 kubeadm.go:582] duration metric: took 32.705665726s to wait for: map[apiserver:true system_pods:true]
I1018 21:10:11.534169   42144 node_conditions.go:102] verifying NodePressure condition ...
I1018 21:10:11.941499   42144 node_conditions.go:122] node storage ephemeral capacity is 93953548Ki
I1018 21:10:11.941536   42144 node_conditions.go:123] node cpu capacity is 4
I1018 21:10:11.941577   42144 node_conditions.go:105] duration metric: took 407.398897ms to run NodePressure ...
I1018 21:10:11.941617   42144 start.go:241] waiting for startup goroutines ...
I1018 21:10:12.341912   42144 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (5.644785574s)
I1018 21:10:16.756941   42144 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1018 21:10:16.965632   42144 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass
I1018 21:10:16.971231   42144 addons.go:510] duration metric: took 38.142710734s for enable addons: enabled=[storage-provisioner default-storageclass]
I1018 21:10:16.971266   42144 start.go:246] waiting for cluster config update ...
I1018 21:10:16.971279   42144 start.go:255] writing updated cluster config ...
I1018 21:10:16.971622   42144 ssh_runner.go:195] Run: rm -f paused
I1018 21:10:17.108737   42144 start.go:600] kubectl: 1.31.1, cluster: 1.31.0 (minor skew: 0)
I1018 21:10:17.114696   42144 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Oct 18 18:25:49 minikube cri-dockerd[824]: time="2024-10-18T18:25:49Z" level=info msg="Pulling image mongo:latest: cca7bf1bde66: Downloading [===============================>                   ]  150.2MB/241.7MB"
Oct 18 18:25:59 minikube cri-dockerd[824]: time="2024-10-18T18:25:59Z" level=info msg="Pulling image mongo:latest: cca7bf1bde66: Downloading [===============================>                   ]  152.8MB/241.7MB"
Oct 18 18:26:09 minikube cri-dockerd[824]: time="2024-10-18T18:26:09Z" level=info msg="Pulling image mongo:latest: cca7bf1bde66: Downloading [===============================>                   ]  154.4MB/241.7MB"
Oct 18 18:26:19 minikube cri-dockerd[824]: time="2024-10-18T18:26:19Z" level=info msg="Pulling image mongo:latest: cca7bf1bde66: Downloading [================================>                  ]  158.2MB/241.7MB"
Oct 18 18:26:29 minikube cri-dockerd[824]: time="2024-10-18T18:26:29Z" level=info msg="Pulling image mongo:latest: cca7bf1bde66: Downloading [=================================>                 ]  163.6MB/241.7MB"
Oct 18 18:26:39 minikube cri-dockerd[824]: time="2024-10-18T18:26:39Z" level=info msg="Pulling image mongo:latest: cca7bf1bde66: Downloading [===================================>               ]    170MB/241.7MB"
Oct 18 18:26:45 minikube cri-dockerd[824]: time="2024-10-18T18:26:45Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/de4150922657630fd8e8e19d71d9dd8f9c3d681cbbe8a85868d1b6158485d75f/resolv.conf as [nameserver 10.96.0.10 search go-depi.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 18 18:26:45 minikube cri-dockerd[824]: time="2024-10-18T18:26:45Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/02422dabc2af230eb9105408a9efea1b45f7cfa0393037c65e71e732a567c24d/resolv.conf as [nameserver 10.96.0.10 search go-depi.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 18 18:26:49 minikube cri-dockerd[824]: time="2024-10-18T18:26:49Z" level=info msg="Pulling image mongo:latest: cca7bf1bde66: Downloading [====================================>              ]  176.4MB/241.7MB"
Oct 18 18:26:59 minikube cri-dockerd[824]: time="2024-10-18T18:26:59Z" level=info msg="Pulling image mongo:latest: cca7bf1bde66: Downloading [=====================================>             ]  181.2MB/241.7MB"
Oct 18 18:27:09 minikube cri-dockerd[824]: time="2024-10-18T18:27:09Z" level=info msg="Pulling image mongo:latest: cca7bf1bde66: Downloading [======================================>            ]  187.1MB/241.7MB"
Oct 18 18:27:19 minikube cri-dockerd[824]: time="2024-10-18T18:27:19Z" level=info msg="Pulling image mongo:latest: cca7bf1bde66: Downloading [=========================================>         ]  198.4MB/241.7MB"
Oct 18 18:27:29 minikube cri-dockerd[824]: time="2024-10-18T18:27:29Z" level=info msg="Pulling image mongo:latest: cca7bf1bde66: Downloading [==========================================>        ]  204.8MB/241.7MB"
Oct 18 18:27:39 minikube cri-dockerd[824]: time="2024-10-18T18:27:39Z" level=info msg="Pulling image mongo:latest: cca7bf1bde66: Downloading [=============================================>     ]  219.3MB/241.7MB"
Oct 18 18:27:49 minikube cri-dockerd[824]: time="2024-10-18T18:27:49Z" level=info msg="Pulling image mongo:latest: cca7bf1bde66: Downloading [==============================================>    ]  224.7MB/241.7MB"
Oct 18 18:27:59 minikube cri-dockerd[824]: time="2024-10-18T18:27:59Z" level=info msg="Pulling image mongo:latest: cca7bf1bde66: Downloading [================================================>  ]  233.8MB/241.7MB"
Oct 18 18:28:09 minikube cri-dockerd[824]: time="2024-10-18T18:28:09Z" level=info msg="Pulling image mongo:latest: cca7bf1bde66: Downloading [=================================================> ]  240.2MB/241.7MB"
Oct 18 18:28:19 minikube cri-dockerd[824]: time="2024-10-18T18:28:19Z" level=info msg="Pulling image mongo:latest: cca7bf1bde66: Extracting [=====================>                             ]  103.6MB/241.7MB"
Oct 18 18:28:29 minikube cri-dockerd[824]: time="2024-10-18T18:28:29Z" level=info msg="Pulling image mongo:latest: cca7bf1bde66: Extracting [=================================================> ]  237.3MB/241.7MB"
Oct 18 18:28:31 minikube cri-dockerd[824]: time="2024-10-18T18:28:31Z" level=info msg="Stop pulling image mongo:latest: Status: Downloaded newer image for mongo:latest"
Oct 18 18:28:35 minikube cri-dockerd[824]: time="2024-10-18T18:28:35Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Oct 18 18:28:36 minikube dockerd[539]: time="2024-10-18T18:28:36.235557442Z" level=info msg="ignoring event" container=5e65810526d9f156d64dde64f627e8b424bc37c682d45aff460ecfa78e1b089b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 18 18:28:36 minikube dockerd[539]: time="2024-10-18T18:28:36.587808277Z" level=info msg="ignoring event" container=ac3e1d110da214e1140928e7c2402c59bb5daac0f27811955f3d9c480909314e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 18 18:28:38 minikube dockerd[539]: time="2024-10-18T18:28:38.209964990Z" level=error msg="Not continuing with pull after error: manifest unknown: manifest unknown"
Oct 18 18:28:41 minikube dockerd[539]: time="2024-10-18T18:28:41.199509627Z" level=error msg="Not continuing with pull after error: manifest unknown: manifest unknown"
Oct 18 18:28:43 minikube cri-dockerd[824]: time="2024-10-18T18:28:43Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Oct 18 18:28:44 minikube dockerd[539]: time="2024-10-18T18:28:44.689944723Z" level=info msg="ignoring event" container=ab546dc91d22a0ada27bc71bf7acc54b675830edd1763d9bf7f011fe727c46f3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 18 18:28:45 minikube dockerd[539]: time="2024-10-18T18:28:45.099905458Z" level=info msg="ignoring event" container=02422dabc2af230eb9105408a9efea1b45f7cfa0393037c65e71e732a567c24d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 18 18:28:45 minikube cri-dockerd[824]: time="2024-10-18T18:28:45Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Oct 18 18:28:46 minikube dockerd[539]: time="2024-10-18T18:28:46.814122359Z" level=info msg="ignoring event" container=0e48f988e085da3a8b92a47b4f5f8b2b380b988aa1f84de36d5d268c098b46fe module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 18 18:28:47 minikube dockerd[539]: time="2024-10-18T18:28:47.205395542Z" level=info msg="ignoring event" container=de4150922657630fd8e8e19d71d9dd8f9c3d681cbbe8a85868d1b6158485d75f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 18 18:29:03 minikube dockerd[539]: time="2024-10-18T18:29:03.115067253Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=84d081c492658f912761d5866b13a333a58e9ee4abf08d10c39df9ac2a9f5aab
Oct 18 18:29:03 minikube dockerd[539]: time="2024-10-18T18:29:03.267004302Z" level=info msg="ignoring event" container=84d081c492658f912761d5866b13a333a58e9ee4abf08d10c39df9ac2a9f5aab module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 18 18:29:03 minikube dockerd[539]: time="2024-10-18T18:29:03.740795378Z" level=info msg="ignoring event" container=c1515aea4dfea8d5e68441c7683f6f019dea3cc6c92e0ff7708d19c1a4dd2251 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 18 18:29:41 minikube cri-dockerd[824]: time="2024-10-18T18:29:41Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2a67b5af202af5bb6c56dc0bdb991ceaf4196ae30363d5cb367e85624e952dd7/resolv.conf as [nameserver 10.96.0.10 search go-depi.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 18 18:29:42 minikube cri-dockerd[824]: time="2024-10-18T18:29:42Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/27f5cbac556831345f76ed562cf26c318a950e48dcc956f184fea04f1c960634/resolv.conf as [nameserver 10.96.0.10 search go-depi.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 18 18:29:48 minikube dockerd[539]: time="2024-10-18T18:29:48.369851743Z" level=error msg="Not continuing with pull after error: manifest unknown: manifest unknown"
Oct 18 18:29:51 minikube dockerd[539]: time="2024-10-18T18:29:51.421037984Z" level=error msg="Not continuing with pull after error: manifest unknown: manifest unknown"
Oct 18 18:29:55 minikube cri-dockerd[824]: time="2024-10-18T18:29:55Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Oct 18 18:29:57 minikube cri-dockerd[824]: time="2024-10-18T18:29:57Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Oct 18 18:30:13 minikube dockerd[539]: time="2024-10-18T18:30:13.951858962Z" level=error msg="Not continuing with pull after error: manifest unknown: manifest unknown"
Oct 18 18:30:16 minikube dockerd[539]: time="2024-10-18T18:30:16.614583405Z" level=error msg="Not continuing with pull after error: manifest unknown: manifest unknown"
Oct 18 18:30:42 minikube dockerd[539]: time="2024-10-18T18:30:42.532807815Z" level=error msg="Not continuing with pull after error: manifest unknown: manifest unknown"
Oct 18 18:30:46 minikube dockerd[539]: time="2024-10-18T18:30:46.218116570Z" level=error msg="Not continuing with pull after error: manifest unknown: manifest unknown"
Oct 18 18:31:33 minikube dockerd[539]: time="2024-10-18T18:31:33.342197287Z" level=error msg="Not continuing with pull after error: manifest unknown: manifest unknown"
Oct 18 18:31:36 minikube dockerd[539]: time="2024-10-18T18:31:36.207265733Z" level=error msg="Not continuing with pull after error: manifest unknown: manifest unknown"
Oct 18 18:33:09 minikube dockerd[539]: time="2024-10-18T18:33:09.096492140Z" level=error msg="Not continuing with pull after error: manifest unknown: manifest unknown"
Oct 18 18:33:12 minikube dockerd[539]: time="2024-10-18T18:33:12.070935897Z" level=error msg="Not continuing with pull after error: manifest unknown: manifest unknown"
Oct 18 18:36:01 minikube dockerd[539]: time="2024-10-18T18:36:01.014177306Z" level=error msg="Not continuing with pull after error: manifest unknown: manifest unknown"
Oct 18 18:36:05 minikube dockerd[539]: time="2024-10-18T18:36:05.264812011Z" level=error msg="Not continuing with pull after error: manifest unknown: manifest unknown"
Oct 18 18:37:55 minikube cri-dockerd[824]: time="2024-10-18T18:37:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1e98afe84afdd664f248f6b29aafceaccff513178bc5f30321a8babd66d538b1/resolv.conf as [nameserver 10.96.0.10 search go-depi.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 18 18:38:09 minikube cri-dockerd[824]: time="2024-10-18T18:38:09Z" level=info msg="Pulling image triple3a/gosurvey:latest: c69f031786c0: Downloading [>                                                  ]  539.9kB/109.9MB"
Oct 18 18:38:19 minikube cri-dockerd[824]: time="2024-10-18T18:38:19Z" level=info msg="Pulling image triple3a/gosurvey:latest: 8663204ce13b: Downloading [================================>                  ]  1.858MB/2.818MB"
Oct 18 18:38:29 minikube cri-dockerd[824]: time="2024-10-18T18:38:29Z" level=info msg="Pulling image triple3a/gosurvey:latest: 7632aaea4354: Downloading [======>                                            ]  897.4kB/7.454MB"
Oct 18 18:38:39 minikube cri-dockerd[824]: time="2024-10-18T18:38:39Z" level=info msg="Pulling image triple3a/gosurvey:latest: 7632aaea4354: Downloading [==============>                                    ]   2.13MB/7.454MB"
Oct 18 18:38:49 minikube cri-dockerd[824]: time="2024-10-18T18:38:49Z" level=info msg="Pulling image triple3a/gosurvey:latest: 7632aaea4354: Downloading [======================>                            ]  3.359MB/7.454MB"
Oct 18 18:38:59 minikube cri-dockerd[824]: time="2024-10-18T18:38:59Z" level=info msg="Pulling image triple3a/gosurvey:latest: 7632aaea4354: Downloading [===========================>                       ]  4.096MB/7.454MB"
Oct 18 18:39:09 minikube cri-dockerd[824]: time="2024-10-18T18:39:09Z" level=info msg="Pulling image triple3a/gosurvey:latest: c69f031786c0: Downloading [===>                                               ]  6.986MB/109.9MB"
Oct 18 18:39:19 minikube cri-dockerd[824]: time="2024-10-18T18:39:19Z" level=info msg="Pulling image triple3a/gosurvey:latest: 7632aaea4354: Downloading [=======================================>           ]   5.89MB/7.454MB"
Oct 18 18:39:29 minikube cri-dockerd[824]: time="2024-10-18T18:39:29Z" level=info msg="Pulling image triple3a/gosurvey:latest: 7632aaea4354: Downloading [==============================================>    ]  6.951MB/7.454MB"


==> container status <==
CONTAINER           IMAGE                                                                           CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
c493d1c3ce729       mongo@sha256:9342a9279a9841fc5f8192e49dcaaf9404e6bb3a90e8cf134eb96074830dd448   9 minutes ago       Running             mongo-app                 0                   27f5cbac55683       go-app-deploy-844d77465d-hvzzq
e8ac40f10b996       mongo@sha256:9342a9279a9841fc5f8192e49dcaaf9404e6bb3a90e8cf134eb96074830dd448   9 minutes ago       Running             mongo-app                 0                   2a67b5af202af       go-app-deploy-844d77465d-jsff4
731121783c98a       6e38f40d628db                                                                   28 minutes ago      Running             storage-provisioner       3                   099c72a9e355f       storage-provisioner
9bc1432c17bb5       cbb01a7bd410d                                                                   28 minutes ago      Running             coredns                   1                   f69d182d67219       coredns-6f6b679f8f-dk69f
e05a3782b9977       ad83b2ca7b09e                                                                   29 minutes ago      Running             kube-proxy                1                   6eca70cecc7a0       kube-proxy-wbwdt
b72b185387ab2       6e38f40d628db                                                                   29 minutes ago      Exited              storage-provisioner       2                   099c72a9e355f       storage-provisioner
88f3c9aea1db3       604f5db92eaa8                                                                   29 minutes ago      Running             kube-apiserver            1                   d35f119e2b979       kube-apiserver-minikube
89cf1e1850096       045733566833c                                                                   29 minutes ago      Running             kube-controller-manager   1                   9ab2173b1e9c2       kube-controller-manager-minikube
619586b2bb15a       1766f54c897f0                                                                   29 minutes ago      Running             kube-scheduler            1                   5ff11fabdfc0a       kube-scheduler-minikube
966e327863ad6       2e96e5913fc06                                                                   29 minutes ago      Running             etcd                      1                   b9557d5b059b2       etcd-minikube
26a264bbcc16b       cbb01a7bd410d                                                                   6 days ago          Exited              coredns                   0                   8c374ed3f0562       coredns-6f6b679f8f-dk69f
889ad675a4ef0       ad83b2ca7b09e                                                                   6 days ago          Exited              kube-proxy                0                   573e9753d18e6       kube-proxy-wbwdt
6dee93b5030df       1766f54c897f0                                                                   6 days ago          Exited              kube-scheduler            0                   97c8cbe83fc09       kube-scheduler-minikube
8daec14a3de6d       2e96e5913fc06                                                                   6 days ago          Exited              etcd                      0                   6faf7c256ec56       etcd-minikube
46ce5f14e7c6c       045733566833c                                                                   6 days ago          Exited              kube-controller-manager   0                   20a7649c4f641       kube-controller-manager-minikube
4151535d12e54       604f5db92eaa8                                                                   6 days ago          Exited              kube-apiserver            0                   7d6017e2cd3e7       kube-apiserver-minikube


==> coredns [26a264bbcc16] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:60888 - 5169 "HINFO IN 973121299915920572.3034297813781888837. udp 56 false 512" - - 0 6.00267558s
[ERROR] plugin/errors: 2 973121299915920572.3034297813781888837. HINFO: read udp 10.244.0.2:38631->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:52181 - 37469 "HINFO IN 973121299915920572.3034297813781888837. udp 56 false 512" - - 0 6.003309329s
[ERROR] plugin/errors: 2 973121299915920572.3034297813781888837. HINFO: read udp 10.244.0.2:54486->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:45126 - 32704 "HINFO IN 973121299915920572.3034297813781888837. udp 56 false 512" - - 0 6.002786822s
[ERROR] plugin/errors: 2 973121299915920572.3034297813781888837. HINFO: read udp 10.244.0.2:56727->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:48292 - 17483 "HINFO IN 973121299915920572.3034297813781888837. udp 56 false 512" - - 0 6.002423441s
[ERROR] plugin/errors: 2 973121299915920572.3034297813781888837. HINFO: read udp 10.244.0.2:46878->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:44342 - 31526 "HINFO IN 973121299915920572.3034297813781888837. udp 56 false 512" - - 0 6.003224923s
[ERROR] plugin/errors: 2 973121299915920572.3034297813781888837. HINFO: read udp 10.244.0.2:44217->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:60608 - 46110 "HINFO IN 973121299915920572.3034297813781888837. udp 56 false 512" - - 0 6.002729438s
[ERROR] plugin/errors: 2 973121299915920572.3034297813781888837. HINFO: read udp 10.244.0.2:34965->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:48257 - 16890 "HINFO IN 973121299915920572.3034297813781888837. udp 56 false 512" - - 0 6.003037939s
[ERROR] plugin/errors: 2 973121299915920572.3034297813781888837. HINFO: read udp 10.244.0.2:35563->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:38223 - 17548 "HINFO IN 973121299915920572.3034297813781888837. udp 56 false 512" - - 0 6.004044332s
[ERROR] plugin/errors: 2 973121299915920572.3034297813781888837. HINFO: read udp 10.244.0.2:43653->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:36904 - 25289 "HINFO IN 973121299915920572.3034297813781888837. udp 56 false 512" - - 0 6.004240014s
[ERROR] plugin/errors: 2 973121299915920572.3034297813781888837. HINFO: read udp 10.244.0.2:40031->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:48740 - 2039 "HINFO IN 973121299915920572.3034297813781888837. udp 56 false 512" - - 0 6.003130218s
[ERROR] plugin/errors: 2 973121299915920572.3034297813781888837. HINFO: read udp 10.244.0.2:48762->192.168.49.1:53: i/o timeout
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> coredns [9bc1432c17bb] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:54783 - 5027 "HINFO IN 8946023349339896801.3858943855271562244. udp 57 false 512" - - 0 6.021089434s
[ERROR] plugin/errors: 2 8946023349339896801.3858943855271562244. HINFO: read udp 10.244.0.3:44186->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:41968 - 61481 "HINFO IN 8946023349339896801.3858943855271562244. udp 57 false 512" - - 0 6.004167442s
[ERROR] plugin/errors: 2 8946023349339896801.3858943855271562244. HINFO: read udp 10.244.0.3:44993->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:42549 - 60842 "HINFO IN 8946023349339896801.3858943855271562244. udp 57 false 512" - - 0 6.004328652s
[ERROR] plugin/errors: 2 8946023349339896801.3858943855271562244. HINFO: read udp 10.244.0.3:41604->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:53656 - 52213 "HINFO IN 8946023349339896801.3858943855271562244. udp 57 false 512" - - 0 6.002758095s
[ERROR] plugin/errors: 2 8946023349339896801.3858943855271562244. HINFO: read udp 10.244.0.3:50952->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:51889 - 62010 "HINFO IN 8946023349339896801.3858943855271562244. udp 57 false 512" - - 0 6.002913426s
[ERROR] plugin/errors: 2 8946023349339896801.3858943855271562244. HINFO: read udp 10.244.0.3:58833->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:32933 - 23246 "HINFO IN 8946023349339896801.3858943855271562244. udp 57 false 512" - - 0 6.003665829s
[ERROR] plugin/errors: 2 8946023349339896801.3858943855271562244. HINFO: read udp 10.244.0.3:52306->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:59005 - 30604 "HINFO IN 8946023349339896801.3858943855271562244. udp 57 false 512" - - 0 6.002999327s
[ERROR] plugin/errors: 2 8946023349339896801.3858943855271562244. HINFO: read udp 10.244.0.3:58682->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:56730 - 54835 "HINFO IN 8946023349339896801.3858943855271562244. udp 57 false 512" - - 0 6.005287757s
[ERROR] plugin/errors: 2 8946023349339896801.3858943855271562244. HINFO: read udp 10.244.0.3:54540->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:46269 - 29637 "HINFO IN 8946023349339896801.3858943855271562244. udp 57 false 512" - - 0 6.004156653s
[ERROR] plugin/errors: 2 8946023349339896801.3858943855271562244. HINFO: read udp 10.244.0.3:55759->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:40943 - 35888 "HINFO IN 8946023349339896801.3858943855271562244. udp 57 false 512" - - 0 6.002761081s
[ERROR] plugin/errors: 2 8946023349339896801.3858943855271562244. HINFO: read udp 10.244.0.3:43429->192.168.49.1:53: i/o timeout


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_10_12T15_47_11_0700
                    minikube.k8s.io/version=v1.34.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sat, 12 Oct 2024 12:47:07 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Fri, 18 Oct 2024 18:39:28 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 18 Oct 2024 18:39:04 +0000   Sat, 12 Oct 2024 12:47:05 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 18 Oct 2024 18:39:04 +0000   Sat, 12 Oct 2024 12:47:05 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 18 Oct 2024 18:39:04 +0000   Sat, 12 Oct 2024 12:47:05 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 18 Oct 2024 18:39:04 +0000   Sat, 12 Oct 2024 12:47:07 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  93953548Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8046744Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  93953548Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8046744Ki
  pods:               110
System Info:
  Machine ID:                 9018f692ec18494aa8ed70a09211912e
  System UUID:                4d49c91f-cff8-4c93-b2c7-2e25ca4b0981
  Boot ID:                    7da7c006-59b1-490d-922b-a9fadd036d13
  Kernel Version:             6.8.0-45-generic
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.2.0
  Kubelet Version:            v1.31.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (10 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  go-depi                     go-app-deploy-7b99dc756b-4764g      0 (0%)        0 (0%)      0 (0%)           0 (0%)         100s
  go-depi                     go-app-deploy-844d77465d-hvzzq      0 (0%)        0 (0%)      0 (0%)           0 (0%)         9m56s
  go-depi                     go-app-deploy-844d77465d-jsff4      0 (0%)        0 (0%)      0 (0%)           0 (0%)         9m56s
  kube-system                 coredns-6f6b679f8f-dk69f            100m (2%)     0 (0%)      70Mi (0%)        170Mi (2%)     6d5h
  kube-system                 etcd-minikube                       100m (2%)     0 (0%)      100Mi (1%)       0 (0%)         6d5h
  kube-system                 kube-apiserver-minikube             250m (6%)     0 (0%)      0 (0%)           0 (0%)         6d5h
  kube-system                 kube-controller-manager-minikube    200m (5%)     0 (0%)      0 (0%)           0 (0%)         6d5h
  kube-system                 kube-proxy-wbwdt                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         6d5h
  kube-system                 kube-scheduler-minikube             100m (2%)     0 (0%)      0 (0%)           0 (0%)         6d5h
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         6d5h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (18%)  0 (0%)
  memory             170Mi (2%)  170Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type    Reason                   Age                  From             Message
  ----    ------                   ----                 ----             -------
  Normal  Starting                 6d5h                 kube-proxy       
  Normal  Starting                 28m                  kube-proxy       
  Normal  NodeHasNoDiskPressure    6d5h (x8 over 6d5h)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientMemory  6d5h (x8 over 6d5h)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  Starting                 6d5h                 kubelet          Starting kubelet.
  Normal  NodeHasSufficientPID     6d5h (x7 over 6d5h)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  6d5h                 kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientPID     6d5h                 kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  6d5h                 kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  6d5h                 kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    6d5h                 kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  Starting                 6d5h                 kubelet          Starting kubelet.
  Normal  RegisteredNode           6d5h                 node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  Starting                 29m                  kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  29m                  kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  29m (x8 over 29m)    kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    29m (x8 over 29m)    kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     29m (x7 over 29m)    kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           29m                  node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[  +0.000003] iwlwifi 0000:03:00.0: iwlwifi device memory mapped registers:
[  +0.000052] iwlwifi 0000:03:00.0: 00000000: 00c00000 00000000 00000000 00000080 00000000 00000000 00000000 00000000
[  +0.000003] iwlwifi 0000:03:00.0: 00000020: 00000001 080003d8 00000210 d55555d5 00000000 d55555d5 80008040 00080042
[  +0.000013] iwlwifi 0000:03:00.0: iwlwifi device AER capability structure:
[  +0.000036] iwlwifi 0000:03:00.0: 00000000: 14010001 00000000 00000000 00462031 00002040 00002000 00000000 00000000
[  +0.000002] iwlwifi 0000:03:00.0: 00000020: 00000000 00000000 00000000
[  +0.000002] iwlwifi 0000:03:00.0: iwlwifi parent port (0000:00:1c.5) config registers:
[  +0.000087] iwlwifi 0000:00:1c.5: 00000000: 9d158086 00100407 060400f1 00810010 00000000 00000000 00030300 200000f0
[  +0.000003] iwlwifi 0000:00:1c.5: 00000020: a200a200 0001fff1 00000000 00000000 00000000 00000040 00000000 0002020a
[  +0.000002] iwlwifi 0000:00:1c.5: 00000040: 01428010 00008001 00110000 06724813 70110042 002cb200 01400000 00000000
[  +0.000003] iwlwifi 0000:00:1c.5: 00000060: 00000000 00000037 00000000 0000000e 00010003 00000000 00000000 00000000
[  +0.000003] iwlwifi 0000:00:1c.5: 00000080: 00019005 fee00278 00000000 00000000 0000a00d 380317aa 00000000 00000000
[  +0.000003] iwlwifi 0000:00:1c.5: 000000a0: c8030001 00000000 00000000 00000000 00000000 00000000 00000000 00000000
[  +0.000003] iwlwifi 0000:00:1c.5: 000000c0: 00000000 00000000 00000000 00000000 07001001 00001842 8b1e0008 00000000
[  +0.000002] iwlwifi 0000:00:1c.5: 000000e0: 00630300 00000000 00100006 00000000 00000150 4c000000 08230fb3 02000004
[  +0.000003] iwlwifi 0000:00:1c.5: 00000100: 14000000 00000000 00010000 00060011 00001001 00002000 00000000 00000000
[  +0.000003] iwlwifi 0000:00:1c.5: 00000120: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
[  +0.000003] iwlwifi 0000:00:1c.5: 00000140: 2001000d 0000000f 0000000d 00000000 00000000 00000000 00000000 00000000
[  +0.000002] iwlwifi 0000:00:1c.5: 00000160: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
[  +0.000003] iwlwifi 0000:00:1c.5: 00000180: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
[  +0.000003] iwlwifi 0000:00:1c.5: 000001a0: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
[  +0.000002] iwlwifi 0000:00:1c.5: 000001c0: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
[  +0.000003] iwlwifi 0000:00:1c.5: 000001e0: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
[  +0.000002] iwlwifi 0000:00:1c.5: 00000200: 2201001e 0028281f 4068280b
[  +0.000035] iwlwifi 0000:03:00.0: reporting RF_KILL (radio enabled)
[  +5.407818] iwlwifi 0000:03:00.0: RF_KILL bit toggled to disable radio.
[  +0.000015] iwlwifi 0000:03:00.0: reporting RF_KILL (radio disabled)
[  +0.043956] iwlwifi 0000:03:00.0: reporting RF_KILL (radio enabled)
[Oct18 12:52] warning: `ThreadPoolForeg' uses wireless extensions which will stop working for Wi-Fi 7 hardware; use nl80211
[Oct18 13:18] kauditd_printk_skb: 3 callbacks suppressed
[Oct18 13:56] kauditd_printk_skb: 24 callbacks suppressed
[Oct18 13:57] kauditd_printk_skb: 97 callbacks suppressed
[Oct18 13:59] workqueue: delayed_fput hogged CPU for >10000us 4 times, consider switching to WQ_UNBOUND
[  +0.317349] kauditd_printk_skb: 4 callbacks suppressed
[Oct18 14:01] kauditd_printk_skb: 6 callbacks suppressed
[Oct18 14:03] workqueue: delayed_fput hogged CPU for >10000us 8 times, consider switching to WQ_UNBOUND
[ +10.998712] workqueue: delayed_fput hogged CPU for >10000us 16 times, consider switching to WQ_UNBOUND
[Oct18 14:12] i915 0000:00:02.0: [drm] *ERROR* CPU pipe A FIFO underrun
[Oct18 14:14] kauditd_printk_skb: 89 callbacks suppressed
[Oct18 14:15] kauditd_printk_skb: 6 callbacks suppressed
[Oct18 14:17] kauditd_printk_skb: 45 callbacks suppressed
[Oct18 16:57] workqueue: delayed_fput hogged CPU for >10000us 32 times, consider switching to WQ_UNBOUND
[Oct18 17:10] kauditd_printk_skb: 6 callbacks suppressed
[  +9.664089] iwlwifi 0000:03:00.0: RF_KILL bit toggled to enable radio.
[  +0.931733] done.
[  +0.008122] thermal thermal_zone3: failed to read out thermal zone (-61)
[Oct18 17:11] sd 2:0:0:0: [sdb] No Caching mode page found
[  +0.000014] sd 2:0:0:0: [sdb] Assuming drive cache: write through
[Oct18 17:12] sd 2:0:0:0: [sdb] No Caching mode page found
[  +0.000006] sd 2:0:0:0: [sdb] Assuming drive cache: write through
[Oct18 17:30] device offline error, dev sdb, sector 64 op 0x1:(WRITE) flags 0x800 phys_seg 1 prio class 0
[  +0.000047] Buffer I/O error on dev sdb1, logical block 0, lost sync page write
[ +10.952839] iwlwifi 0000:03:00.0: RF_KILL bit toggled to enable radio.
[  +0.938276] done.
[  +0.013079] thermal thermal_zone3: failed to read out thermal zone (-61)
[Oct18 17:53] i915 0000:00:02.0: [drm] *ERROR* CPU pipe A FIFO underrun
[Oct18 17:54] iwlwifi 0000:03:00.0: RF_KILL bit toggled to enable radio.
[  +0.873263] done.
[  +0.008544] thermal thermal_zone3: failed to read out thermal zone (-61)
[Oct18 18:06] i915 0000:00:02.0: [drm] *ERROR* CPU pipe A FIFO underrun


==> etcd [8daec14a3de6] <==
{"level":"info","ts":"2024-10-12T13:07:06.116112Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":1119,"took":"9.317464ms","hash":4270151030,"current-db-size-bytes":1413120,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":905216,"current-db-size-in-use":"905 kB"}
{"level":"info","ts":"2024-10-12T13:07:06.116239Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4270151030,"revision":1119,"compact-revision":879}
{"level":"info","ts":"2024-10-12T13:12:06.121642Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1359}
{"level":"info","ts":"2024-10-12T13:12:06.131605Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":1359,"took":"9.087299ms","hash":3674478254,"current-db-size-bytes":1413120,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":901120,"current-db-size-in-use":"901 kB"}
{"level":"info","ts":"2024-10-12T13:12:06.131698Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3674478254,"revision":1359,"compact-revision":1119}
{"level":"info","ts":"2024-10-12T13:17:06.135332Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1597}
{"level":"info","ts":"2024-10-12T13:17:06.144529Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":1597,"took":"8.640018ms","hash":3182460861,"current-db-size-bytes":1413120,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":897024,"current-db-size-in-use":"897 kB"}
{"level":"info","ts":"2024-10-12T13:17:06.144613Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3182460861,"revision":1597,"compact-revision":1359}
{"level":"info","ts":"2024-10-12T13:22:06.149065Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1837}
{"level":"info","ts":"2024-10-12T13:22:06.158567Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":1837,"took":"8.899257ms","hash":2869608536,"current-db-size-bytes":1413120,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":897024,"current-db-size-in-use":"897 kB"}
{"level":"info","ts":"2024-10-12T13:22:06.158660Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2869608536,"revision":1837,"compact-revision":1597}
{"level":"info","ts":"2024-10-12T13:27:06.163627Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2077}
{"level":"info","ts":"2024-10-12T13:27:06.173085Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":2077,"took":"8.663773ms","hash":3100328547,"current-db-size-bytes":1413120,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":901120,"current-db-size-in-use":"901 kB"}
{"level":"info","ts":"2024-10-12T13:27:06.173225Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3100328547,"revision":2077,"compact-revision":1837}
{"level":"warn","ts":"2024-10-12T23:13:07.765614Z","caller":"etcdserver/v3_server.go:920","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128032509249811677,"retry-timeout":"500ms"}
{"level":"warn","ts":"2024-10-12T23:13:08.266252Z","caller":"etcdserver/v3_server.go:920","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128032509249811677,"retry-timeout":"500ms"}
{"level":"warn","ts":"2024-10-12T23:13:08.767320Z","caller":"etcdserver/v3_server.go:920","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128032509249811677,"retry-timeout":"500ms"}
{"level":"warn","ts":"2024-10-12T23:13:09.265681Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"2.00006566s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"","error":"context deadline exceeded"}
{"level":"info","ts":"2024-10-12T23:13:09.265825Z","caller":"traceutil/trace.go:171","msg":"trace[157743232] range","detail":"{range_begin:/registry/health; range_end:; }","duration":"2.000251791s","start":"2024-10-12T23:13:07.265539Z","end":"2024-10-12T23:13:09.265791Z","steps":["trace[157743232] 'agreement among raft nodes before linearized reading'  (duration: 2.000059206s)"],"step_count":1}
{"level":"warn","ts":"2024-10-12T23:13:09.265934Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-12T23:13:07.265389Z","time spent":"2.000519315s","remote":"127.0.0.1:34178","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":0,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-10-12T23:13:09.268030Z","caller":"etcdserver/v3_server.go:920","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128032509249811677,"retry-timeout":"500ms"}
{"level":"warn","ts":"2024-10-12T23:13:09.769189Z","caller":"etcdserver/v3_server.go:920","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128032509249811677,"retry-timeout":"500ms"}
{"level":"warn","ts":"2024-10-12T23:13:10.269947Z","caller":"etcdserver/v3_server.go:920","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128032509249811677,"retry-timeout":"500ms"}
{"level":"warn","ts":"2024-10-12T23:13:10.770666Z","caller":"etcdserver/v3_server.go:920","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128032509249811677,"retry-timeout":"500ms"}
{"level":"warn","ts":"2024-10-12T23:13:10.828721Z","caller":"wal/wal.go:805","msg":"slow fdatasync","took":"3.570469474s","expected-duration":"1s"}
{"level":"warn","ts":"2024-10-12T23:13:10.829494Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-12T23:13:07.258082Z","time spent":"3.571401315s","remote":"127.0.0.1:34204","response type":"/etcdserverpb.Lease/LeaseGrant","request count":-1,"request size":-1,"response count":-1,"response size":-1,"request content":""}
{"level":"info","ts":"2024-10-12T23:13:11.047868Z","caller":"traceutil/trace.go:171","msg":"trace[2008027527] linearizableReadLoop","detail":"{readStateIndex:2969; appliedIndex:2967; }","duration":"3.782282791s","start":"2024-10-12T23:13:07.265544Z","end":"2024-10-12T23:13:11.047827Z","steps":["trace[2008027527] 'read index received'  (duration: 3.564128922s)","trace[2008027527] 'applied index is now lower than readState.Index'  (duration: 218.151741ms)"],"step_count":2}
{"level":"info","ts":"2024-10-12T23:13:11.048505Z","caller":"traceutil/trace.go:171","msg":"trace[1948978045] transaction","detail":"{read_only:false; response_revision:2439; number_of_response:1; }","duration":"3.784412874s","start":"2024-10-12T23:13:07.264056Z","end":"2024-10-12T23:13:11.048468Z","steps":["trace[1948978045] 'process raft request'  (duration: 3.783353576s)"],"step_count":1}
{"level":"warn","ts":"2024-10-12T23:13:11.048787Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-12T23:13:07.264038Z","time spent":"3.784576116s","remote":"127.0.0.1:34358","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":585,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:2438 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:512 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2024-10-12T23:13:11.049521Z","caller":"traceutil/trace.go:171","msg":"trace[776278144] transaction","detail":"{read_only:false; response_revision:2440; number_of_response:1; }","duration":"2.472417456s","start":"2024-10-12T23:13:08.577068Z","end":"2024-10-12T23:13:11.049485Z","steps":["trace[776278144] 'process raft request'  (duration: 2.470627968s)"],"step_count":1}
{"level":"warn","ts":"2024-10-12T23:13:11.052795Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-12T23:13:08.577023Z","time spent":"2.472578918s","remote":"127.0.0.1:34458","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:2433 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"warn","ts":"2024-10-12T23:13:11.068108Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"3.685057067s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/prioritylevelconfigurations/\" range_end:\"/registry/prioritylevelconfigurations0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2024-10-12T23:13:11.081155Z","caller":"traceutil/trace.go:171","msg":"trace[951256587] range","detail":"{range_begin:/registry/prioritylevelconfigurations/; range_end:/registry/prioritylevelconfigurations0; response_count:0; response_revision:2441; }","duration":"3.698121402s","start":"2024-10-12T23:13:07.383007Z","end":"2024-10-12T23:13:11.081128Z","steps":["trace[951256587] 'agreement among raft nodes before linearized reading'  (duration: 3.684967572s)"],"step_count":1}
{"level":"warn","ts":"2024-10-12T23:13:11.073879Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"550.729468ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/\" range_end:\"/registry/services/endpoints0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2024-10-12T23:13:11.073966Z","caller":"traceutil/trace.go:171","msg":"trace[970185122] transaction","detail":"{read_only:false; response_revision:2441; number_of_response:1; }","duration":"242.071701ms","start":"2024-10-12T23:13:10.831871Z","end":"2024-10-12T23:13:11.073943Z","steps":["trace[970185122] 'process raft request'  (duration: 235.965861ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-12T23:13:11.074054Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"222.851372ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2024-10-12T23:13:11.074129Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.806951176s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2024-10-12T23:13:11.074182Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.801006424s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/volumeattachments/\" range_end:\"/registry/volumeattachments0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2024-10-12T23:13:11.074250Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"3.043904648s","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2024-10-12T23:13:11.084683Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-12T23:13:07.382957Z","time spent":"3.699753061s","remote":"127.0.0.1:34632","response type":"/etcdserverpb.KV/Range","request count":0,"request size":82,"response count":8,"response size":31,"request content":"key:\"/registry/prioritylevelconfigurations/\" range_end:\"/registry/prioritylevelconfigurations0\" count_only:true "}
{"level":"info","ts":"2024-10-12T23:13:11.085710Z","caller":"traceutil/trace.go:171","msg":"trace[864097055] range","detail":"{range_begin:/registry/services/endpoints/; range_end:/registry/services/endpoints0; response_count:0; response_revision:2441; }","duration":"562.578725ms","start":"2024-10-12T23:13:10.523109Z","end":"2024-10-12T23:13:11.085688Z","steps":["trace[864097055] 'agreement among raft nodes before linearized reading'  (duration: 550.67314ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-12T23:13:11.086810Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-12T23:13:10.523020Z","time spent":"563.680724ms","remote":"127.0.0.1:34358","response type":"/etcdserverpb.KV/Range","request count":0,"request size":64,"response count":3,"response size":31,"request content":"key:\"/registry/services/endpoints/\" range_end:\"/registry/services/endpoints0\" count_only:true "}
{"level":"info","ts":"2024-10-12T23:13:11.088601Z","caller":"traceutil/trace.go:171","msg":"trace[88096466] range","detail":"{range_begin:/registry/volumeattachments/; range_end:/registry/volumeattachments0; response_count:0; response_revision:2441; }","duration":"1.815418021s","start":"2024-10-12T23:13:09.273162Z","end":"2024-10-12T23:13:11.088580Z","steps":["trace[88096466] 'agreement among raft nodes before linearized reading'  (duration: 1.800990301s)"],"step_count":1}
{"level":"warn","ts":"2024-10-12T23:13:11.088697Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-12T23:13:09.273073Z","time spent":"1.815593365s","remote":"127.0.0.1:34586","response type":"/etcdserverpb.KV/Range","request count":0,"request size":62,"response count":0,"response size":29,"request content":"key:\"/registry/volumeattachments/\" range_end:\"/registry/volumeattachments0\" count_only:true "}
{"level":"info","ts":"2024-10-12T23:13:11.088848Z","caller":"traceutil/trace.go:171","msg":"trace[255774999] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:2441; }","duration":"3.058499958s","start":"2024-10-12T23:13:08.030334Z","end":"2024-10-12T23:13:11.088834Z","steps":["trace[255774999] 'agreement among raft nodes before linearized reading'  (duration: 3.043892247s)"],"step_count":1}
{"level":"info","ts":"2024-10-12T23:13:11.088876Z","caller":"traceutil/trace.go:171","msg":"trace[942775282] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:2441; }","duration":"237.679454ms","start":"2024-10-12T23:13:10.851186Z","end":"2024-10-12T23:13:11.088866Z","steps":["trace[942775282] 'agreement among raft nodes before linearized reading'  (duration: 222.819672ms)"],"step_count":1}
{"level":"info","ts":"2024-10-12T23:13:11.089026Z","caller":"traceutil/trace.go:171","msg":"trace[2041935114] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:2441; }","duration":"1.821849714s","start":"2024-10-12T23:13:09.267166Z","end":"2024-10-12T23:13:11.089016Z","steps":["trace[2041935114] 'agreement among raft nodes before linearized reading'  (duration: 1.806935617s)"],"step_count":1}
{"level":"warn","ts":"2024-10-12T23:13:11.089063Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-12T23:13:09.267065Z","time spent":"1.821987514s","remote":"127.0.0.1:34174","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-10-12T23:13:26.404075Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"107.356724ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128032509249811768 > lease_revoke:<id:70cc9280c4a974de>","response":"size:29"}
{"level":"info","ts":"2024-10-12T23:13:31.512005Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-10-12T23:13:31.512064Z","caller":"embed/etcd.go:377","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2024-10-12T23:13:31.512179Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-10-12T23:13:31.512333Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
2024/10/12 23:13:31 WARNING: [core] [Server #7] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
{"level":"warn","ts":"2024-10-12T23:13:31.602847Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-10-12T23:13:31.602909Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2024-10-12T23:13:31.603038Z","caller":"etcdserver/server.go:1521","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-10-12T23:13:31.730066Z","caller":"embed/etcd.go:581","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-10-12T23:13:31.730367Z","caller":"embed/etcd.go:586","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-10-12T23:13:31.730389Z","caller":"embed/etcd.go:379","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> etcd [966e327863ad] <==
{"level":"warn","ts":"2024-10-18T18:28:16.730355Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"229.049074ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128032646919762541 > lease_revoke:<id:70cc92a0d26e3218>","response":"size:29"}
{"level":"info","ts":"2024-10-18T18:28:16.730789Z","caller":"traceutil/trace.go:171","msg":"trace[2035587399] linearizableReadLoop","detail":"{readStateIndex:4212; appliedIndex:4211; }","duration":"321.254864ms","start":"2024-10-18T18:28:16.409496Z","end":"2024-10-18T18:28:16.730751Z","steps":["trace[2035587399] 'read index received'  (duration: 91.710207ms)","trace[2035587399] 'applied index is now lower than readState.Index'  (duration: 229.541933ms)"],"step_count":2}
{"level":"warn","ts":"2024-10-18T18:28:16.731273Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"321.769073ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/horizontalpodautoscalers/\" range_end:\"/registry/horizontalpodautoscalers0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-10-18T18:28:16.731362Z","caller":"traceutil/trace.go:171","msg":"trace[210081005] range","detail":"{range_begin:/registry/horizontalpodautoscalers/; range_end:/registry/horizontalpodautoscalers0; response_count:0; response_revision:3446; }","duration":"322.054105ms","start":"2024-10-18T18:28:16.409283Z","end":"2024-10-18T18:28:16.731337Z","steps":["trace[210081005] 'agreement among raft nodes before linearized reading'  (duration: 321.912564ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-18T18:28:16.731456Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-18T18:28:16.409192Z","time spent":"322.23478ms","remote":"127.0.0.1:39472","response type":"/etcdserverpb.KV/Range","request count":0,"request size":76,"response count":0,"response size":29,"request content":"key:\"/registry/horizontalpodautoscalers/\" range_end:\"/registry/horizontalpodautoscalers0\" count_only:true "}
{"level":"warn","ts":"2024-10-18T18:28:16.732449Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"299.961479ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-10-18T18:28:16.732535Z","caller":"traceutil/trace.go:171","msg":"trace[1257895091] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:3446; }","duration":"300.043836ms","start":"2024-10-18T18:28:16.432465Z","end":"2024-10-18T18:28:16.732509Z","steps":["trace[1257895091] 'agreement among raft nodes before linearized reading'  (duration: 299.922949ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-18T18:28:16.734463Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-18T18:28:16.432378Z","time spent":"302.047769ms","remote":"127.0.0.1:39228","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-10-18T18:28:18.692702Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"254.36236ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-10-18T18:28:18.692841Z","caller":"traceutil/trace.go:171","msg":"trace[1218642857] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:3447; }","duration":"254.513488ms","start":"2024-10-18T18:28:18.438295Z","end":"2024-10-18T18:28:18.692809Z","steps":["trace[1218642857] 'range keys from in-memory index tree'  (duration: 254.182539ms)"],"step_count":1}
{"level":"info","ts":"2024-10-18T18:28:21.391268Z","caller":"traceutil/trace.go:171","msg":"trace[1081195073] linearizableReadLoop","detail":"{readStateIndex:4215; appliedIndex:4214; }","duration":"111.365297ms","start":"2024-10-18T18:28:21.279865Z","end":"2024-10-18T18:28:21.391230Z","steps":["trace[1081195073] 'read index received'  (duration: 111.041008ms)","trace[1081195073] 'applied index is now lower than readState.Index'  (duration: 322.142¬µs)"],"step_count":2}
{"level":"warn","ts":"2024-10-18T18:28:21.391515Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"111.629445ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterroles/\" range_end:\"/registry/clusterroles0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2024-10-18T18:28:21.391623Z","caller":"traceutil/trace.go:171","msg":"trace[435775510] range","detail":"{range_begin:/registry/clusterroles/; range_end:/registry/clusterroles0; response_count:0; response_revision:3448; }","duration":"111.731855ms","start":"2024-10-18T18:28:21.279845Z","end":"2024-10-18T18:28:21.391577Z","steps":["trace[435775510] 'agreement among raft nodes before linearized reading'  (duration: 111.551857ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-18T18:28:21.738790Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"105.588353ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2024-10-18T18:28:21.739403Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"244.027379ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128032646919762564 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:3441 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128032646919762561 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:16"}
{"level":"info","ts":"2024-10-18T18:28:21.739723Z","caller":"traceutil/trace.go:171","msg":"trace[104197812] linearizableReadLoop","detail":"{readStateIndex:4216; appliedIndex:4215; }","duration":"302.338438ms","start":"2024-10-18T18:28:21.437170Z","end":"2024-10-18T18:28:21.739509Z","steps":["trace[104197812] 'read index received'  (duration: 57.668098ms)","trace[104197812] 'applied index is now lower than readState.Index'  (duration: 244.666978ms)"],"step_count":2}
{"level":"info","ts":"2024-10-18T18:28:21.739713Z","caller":"traceutil/trace.go:171","msg":"trace[435344215] transaction","detail":"{read_only:false; response_revision:3449; number_of_response:1; }","duration":"344.838225ms","start":"2024-10-18T18:28:21.394834Z","end":"2024-10-18T18:28:21.739672Z","steps":["trace[435344215] 'process raft request'  (duration: 100.088328ms)","trace[435344215] 'compare'  (duration: 243.853398ms)"],"step_count":2}
{"level":"warn","ts":"2024-10-18T18:28:21.739907Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-18T18:28:21.394798Z","time spent":"345.026121ms","remote":"127.0.0.1:39252","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":116,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:3441 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128032646919762561 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >"}
{"level":"warn","ts":"2024-10-18T18:28:21.739975Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"302.81696ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-10-18T18:28:21.739990Z","caller":"traceutil/trace.go:171","msg":"trace[1973239651] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:3448; }","duration":"106.787872ms","start":"2024-10-18T18:28:21.633165Z","end":"2024-10-18T18:28:21.739953Z","steps":["trace[1973239651] 'range keys from in-memory index tree'  (duration: 105.556713ms)"],"step_count":1}
{"level":"info","ts":"2024-10-18T18:28:21.740052Z","caller":"traceutil/trace.go:171","msg":"trace[1575107931] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:3449; }","duration":"302.899353ms","start":"2024-10-18T18:28:21.437132Z","end":"2024-10-18T18:28:21.740031Z","steps":["trace[1575107931] 'agreement among raft nodes before linearized reading'  (duration: 302.67ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-18T18:28:21.740125Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-18T18:28:21.437030Z","time spent":"303.079077ms","remote":"127.0.0.1:39228","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2024-10-18T18:28:24.412543Z","caller":"traceutil/trace.go:171","msg":"trace[416313114] transaction","detail":"{read_only:false; response_revision:3452; number_of_response:1; }","duration":"462.980299ms","start":"2024-10-18T18:28:23.949523Z","end":"2024-10-18T18:28:24.412503Z","steps":["trace[416313114] 'process raft request'  (duration: 462.686484ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-18T18:28:24.413066Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-18T18:28:23.949478Z","time spent":"463.387454ms","remote":"127.0.0.1:39432","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:3451 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2024-10-18T18:28:25.047533Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"458.834388ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128032646919762583 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:3445 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >>","response":"size:16"}
{"level":"info","ts":"2024-10-18T18:28:25.047743Z","caller":"traceutil/trace.go:171","msg":"trace[1980344183] linearizableReadLoop","detail":"{readStateIndex:4220; appliedIndex:4219; }","duration":"414.71928ms","start":"2024-10-18T18:28:24.632993Z","end":"2024-10-18T18:28:25.047712Z","steps":["trace[1980344183] 'read index received'  (duration: 80.038¬µs)","trace[1980344183] 'applied index is now lower than readState.Index'  (duration: 414.633299ms)"],"step_count":2}
{"level":"info","ts":"2024-10-18T18:28:25.047920Z","caller":"traceutil/trace.go:171","msg":"trace[1681732046] transaction","detail":"{read_only:false; response_revision:3453; number_of_response:1; }","duration":"1.043551769s","start":"2024-10-18T18:28:24.004339Z","end":"2024-10-18T18:28:25.047891Z","steps":["trace[1681732046] 'process raft request'  (duration: 584.211016ms)","trace[1681732046] 'compare'  (duration: 458.661612ms)"],"step_count":2}
{"level":"warn","ts":"2024-10-18T18:28:25.048098Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-18T18:28:24.004297Z","time spent":"1.043688172s","remote":"127.0.0.1:39524","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:3445 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"warn","ts":"2024-10-18T18:28:25.048509Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"313.466309ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/namespaces/\" range_end:\"/registry/namespaces0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2024-10-18T18:28:25.048639Z","caller":"traceutil/trace.go:171","msg":"trace[325784471] range","detail":"{range_begin:/registry/namespaces/; range_end:/registry/namespaces0; response_count:0; response_revision:3453; }","duration":"313.578503ms","start":"2024-10-18T18:28:24.735009Z","end":"2024-10-18T18:28:25.048587Z","steps":["trace[325784471] 'agreement among raft nodes before linearized reading'  (duration: 313.42112ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-18T18:28:25.048738Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-18T18:28:24.734911Z","time spent":"313.794284ms","remote":"127.0.0.1:39368","response type":"/etcdserverpb.KV/Range","request count":0,"request size":48,"response count":5,"response size":31,"request content":"key:\"/registry/namespaces/\" range_end:\"/registry/namespaces0\" count_only:true "}
{"level":"warn","ts":"2024-10-18T18:28:25.048899Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"415.893947ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-10-18T18:28:25.049004Z","caller":"traceutil/trace.go:171","msg":"trace[1599986494] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:3453; }","duration":"416.022821ms","start":"2024-10-18T18:28:24.632956Z","end":"2024-10-18T18:28:25.048979Z","steps":["trace[1599986494] 'agreement among raft nodes before linearized reading'  (duration: 415.855406ms)"],"step_count":1}
{"level":"info","ts":"2024-10-18T18:28:30.958005Z","caller":"traceutil/trace.go:171","msg":"trace[980674297] linearizableReadLoop","detail":"{readStateIndex:4224; appliedIndex:4223; }","duration":"325.16638ms","start":"2024-10-18T18:28:30.632803Z","end":"2024-10-18T18:28:30.957969Z","steps":["trace[980674297] 'read index received'  (duration: 325.007927ms)","trace[980674297] 'applied index is now lower than readState.Index'  (duration: 156.694¬µs)"],"step_count":2}
{"level":"info","ts":"2024-10-18T18:28:30.958171Z","caller":"traceutil/trace.go:171","msg":"trace[131431911] transaction","detail":"{read_only:false; response_revision:3456; number_of_response:1; }","duration":"414.335693ms","start":"2024-10-18T18:28:30.543800Z","end":"2024-10-18T18:28:30.958136Z","steps":["trace[131431911] 'process raft request'  (duration: 413.977465ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-18T18:28:30.958242Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"325.411005ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-10-18T18:28:30.958321Z","caller":"traceutil/trace.go:171","msg":"trace[1160661607] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:3456; }","duration":"325.506261ms","start":"2024-10-18T18:28:30.632793Z","end":"2024-10-18T18:28:30.958299Z","steps":["trace[1160661607] 'agreement among raft nodes before linearized reading'  (duration: 325.382122ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-18T18:28:30.958384Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-18T18:28:30.543766Z","time spent":"414.497851ms","remote":"127.0.0.1:39432","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:3455 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2024-10-18T18:28:31.464675Z","caller":"traceutil/trace.go:171","msg":"trace[1334767400] transaction","detail":"{read_only:false; response_revision:3457; number_of_response:1; }","duration":"144.563244ms","start":"2024-10-18T18:28:31.320028Z","end":"2024-10-18T18:28:31.464591Z","steps":["trace[1334767400] 'process raft request'  (duration: 87.317342ms)","trace[1334767400] 'compare'  (duration: 56.774817ms)"],"step_count":2}
{"level":"info","ts":"2024-10-18T18:28:32.065373Z","caller":"traceutil/trace.go:171","msg":"trace[1840159153] transaction","detail":"{read_only:false; response_revision:3459; number_of_response:1; }","duration":"158.617182ms","start":"2024-10-18T18:28:31.906712Z","end":"2024-10-18T18:28:32.065330Z","steps":["trace[1840159153] 'process raft request'  (duration: 158.22355ms)"],"step_count":1}
{"level":"info","ts":"2024-10-18T18:29:40.607215Z","caller":"traceutil/trace.go:171","msg":"trace[1580634231] transaction","detail":"{read_only:false; response_revision:3582; number_of_response:1; }","duration":"611.223206ms","start":"2024-10-18T18:29:39.995944Z","end":"2024-10-18T18:29:40.607167Z","steps":["trace[1580634231] 'process raft request'  (duration: 610.983924ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-18T18:29:40.607519Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-18T18:29:39.995903Z","time spent":"611.47211ms","remote":"127.0.0.1:39432","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:3553 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2024-10-18T18:29:41.071101Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"313.243802ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/validatingadmissionpolicies/\" range_end:\"/registry/validatingadmissionpolicies0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-10-18T18:29:41.071292Z","caller":"traceutil/trace.go:171","msg":"trace[984500394] range","detail":"{range_begin:/registry/validatingadmissionpolicies/; range_end:/registry/validatingadmissionpolicies0; response_count:0; response_revision:3582; }","duration":"313.462584ms","start":"2024-10-18T18:29:40.757784Z","end":"2024-10-18T18:29:41.071246Z","steps":["trace[984500394] 'count revisions from in-memory index tree'  (duration: 313.103986ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-18T18:29:41.071441Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-18T18:29:40.757707Z","time spent":"313.683633ms","remote":"127.0.0.1:39760","response type":"/etcdserverpb.KV/Range","request count":0,"request size":82,"response count":0,"response size":29,"request content":"key:\"/registry/validatingadmissionpolicies/\" range_end:\"/registry/validatingadmissionpolicies0\" count_only:true "}
{"level":"info","ts":"2024-10-18T18:30:07.833064Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3263}
{"level":"info","ts":"2024-10-18T18:30:07.851514Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":3263,"took":"17.662212ms","hash":3750125052,"current-db-size-bytes":2301952,"current-db-size":"2.3 MB","current-db-size-in-use-bytes":2088960,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2024-10-18T18:30:07.851710Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3750125052,"revision":3263,"compact-revision":3024}
{"level":"warn","ts":"2024-10-18T18:31:56.608757Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"144.925079ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-10-18T18:31:56.608956Z","caller":"traceutil/trace.go:171","msg":"trace[2054557995] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:3762; }","duration":"145.129014ms","start":"2024-10-18T18:31:56.463787Z","end":"2024-10-18T18:31:56.608916Z","steps":["trace[2054557995] 'range keys from in-memory index tree'  (duration: 144.788868ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-18T18:31:57.040450Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"219.960361ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128032646919763882 > lease_revoke:<id:70cc92a0d26e3755>","response":"size:29"}
{"level":"info","ts":"2024-10-18T18:31:57.040901Z","caller":"traceutil/trace.go:171","msg":"trace[1674477884] transaction","detail":"{read_only:false; response_revision:3763; number_of_response:1; }","duration":"270.423826ms","start":"2024-10-18T18:31:56.770444Z","end":"2024-10-18T18:31:57.040867Z","steps":["trace[1674477884] 'process raft request'  (duration: 270.223078ms)"],"step_count":1}
{"level":"info","ts":"2024-10-18T18:31:57.040991Z","caller":"traceutil/trace.go:171","msg":"trace[1339053772] linearizableReadLoop","detail":"{readStateIndex:4579; appliedIndex:4578; }","duration":"285.880448ms","start":"2024-10-18T18:31:56.755028Z","end":"2024-10-18T18:31:57.040908Z","steps":["trace[1339053772] 'read index received'  (duration: 65.693837ms)","trace[1339053772] 'applied index is now lower than readState.Index'  (duration: 220.182734ms)"],"step_count":2}
{"level":"warn","ts":"2024-10-18T18:31:57.041331Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"286.270195ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/go-depi/go-app-deploy-844d77465d-jsff4\" ","response":"range_response_count:1 size:4013"}
{"level":"info","ts":"2024-10-18T18:31:57.041453Z","caller":"traceutil/trace.go:171","msg":"trace[114200222] range","detail":"{range_begin:/registry/pods/go-depi/go-app-deploy-844d77465d-jsff4; range_end:; response_count:1; response_revision:3763; }","duration":"286.406066ms","start":"2024-10-18T18:31:56.755017Z","end":"2024-10-18T18:31:57.041423Z","steps":["trace[114200222] 'agreement among raft nodes before linearized reading'  (duration: 286.147949ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-18T18:31:57.041873Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"204.073411ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/priorityclasses/\" range_end:\"/registry/priorityclasses0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2024-10-18T18:31:57.041959Z","caller":"traceutil/trace.go:171","msg":"trace[52188454] range","detail":"{range_begin:/registry/priorityclasses/; range_end:/registry/priorityclasses0; response_count:0; response_revision:3763; }","duration":"204.171258ms","start":"2024-10-18T18:31:56.837765Z","end":"2024-10-18T18:31:57.041936Z","steps":["trace[52188454] 'agreement among raft nodes before linearized reading'  (duration: 204.037618ms)"],"step_count":1}
{"level":"info","ts":"2024-10-18T18:35:07.848872Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3639}
{"level":"info","ts":"2024-10-18T18:35:07.860381Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":3639,"took":"10.726768ms","hash":1323565351,"current-db-size-bytes":2301952,"current-db-size":"2.3 MB","current-db-size-in-use-bytes":2273280,"current-db-size-in-use":"2.3 MB"}
{"level":"info","ts":"2024-10-18T18:35:07.860470Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1323565351,"revision":3639,"compact-revision":3263}


==> kernel <==
 18:39:34 up 1 day,  2:15,  0 users,  load average: 3.35, 2.67, 2.13
Linux minikube 6.8.0-45-generic #45~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Sep 11 15:25:05 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [4151535d12e5] <==
W1012 23:13:36.996326       1 logging.go:55] [core] [Channel #148 SubChannel #149]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:37.037750       1 logging.go:55] [core] [Channel #118 SubChannel #119]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:37.075365       1 logging.go:55] [core] [Channel #160 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:37.083270       1 logging.go:55] [core] [Channel #139 SubChannel #140]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:37.126729       1 logging.go:55] [core] [Channel #112 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:37.161489       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:37.310920       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:37.382777       1 logging.go:55] [core] [Channel #166 SubChannel #167]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:37.397881       1 logging.go:55] [core] [Channel #127 SubChannel #128]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:37.510795       1 logging.go:55] [core] [Channel #64 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:39.480778       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:39.578466       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:39.639405       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:39.669969       1 logging.go:55] [core] [Channel #40 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:39.879200       1 logging.go:55] [core] [Channel #133 SubChannel #134]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:39.990244       1 logging.go:55] [core] [Channel #136 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:40.035253       1 logging.go:55] [core] [Channel #124 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:40.116826       1 logging.go:55] [core] [Channel #37 SubChannel #38]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:40.223467       1 logging.go:55] [core] [Channel #154 SubChannel #155]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:40.284349       1 logging.go:55] [core] [Channel #88 SubChannel #89]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:40.307516       1 logging.go:55] [core] [Channel #169 SubChannel #170]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:40.318417       1 logging.go:55] [core] [Channel #157 SubChannel #158]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:40.319333       1 logging.go:55] [core] [Channel #17 SubChannel #18]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:40.331886       1 logging.go:55] [core] [Channel #145 SubChannel #146]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:40.338132       1 logging.go:55] [core] [Channel #76 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:40.367753       1 logging.go:55] [core] [Channel #25 SubChannel #26]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:40.391709       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:40.411387       1 logging.go:55] [core] [Channel #61 SubChannel #62]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:40.451527       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:40.611058       1 logging.go:55] [core] [Channel #172 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:40.617093       1 logging.go:55] [core] [Channel #178 SubChannel #179]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:40.654752       1 logging.go:55] [core] [Channel #85 SubChannel #86]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:40.660399       1 logging.go:55] [core] [Channel #49 SubChannel #50]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:40.707032       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:40.714124       1 logging.go:55] [core] [Channel #28 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:40.732017       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:40.744943       1 logging.go:55] [core] [Channel #181 SubChannel #182]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:40.745048       1 logging.go:55] [core] [Channel #67 SubChannel #68]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:40.780290       1 logging.go:55] [core] [Channel #1 SubChannel #4]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:40.787768       1 logging.go:55] [core] [Channel #73 SubChannel #74]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:40.791419       1 logging.go:55] [core] [Channel #34 SubChannel #35]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:40.806799       1 logging.go:55] [core] [Channel #142 SubChannel #143]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:40.850761       1 logging.go:55] [core] [Channel #2 SubChannel #3]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:40.852404       1 logging.go:55] [core] [Channel #94 SubChannel #95]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:40.863680       1 logging.go:55] [core] [Channel #109 SubChannel #110]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:40.919906       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:40.948086       1 logging.go:55] [core] [Channel #106 SubChannel #107]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:40.960107       1 logging.go:55] [core] [Channel #100 SubChannel #101]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:40.964941       1 logging.go:55] [core] [Channel #112 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:41.080152       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:41.100058       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:41.109740       1 logging.go:55] [core] [Channel #22 SubChannel #23]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:41.135796       1 logging.go:55] [core] [Channel #46 SubChannel #47]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:41.144740       1 logging.go:55] [core] [Channel #121 SubChannel #122]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:41.166238       1 logging.go:55] [core] [Channel #82 SubChannel #83]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:41.177066       1 logging.go:55] [core] [Channel #166 SubChannel #167]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:41.218402       1 logging.go:55] [core] [Channel #31 SubChannel #32]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:41.232512       1 logging.go:55] [core] [Channel #97 SubChannel #98]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:41.277715       1 logging.go:55] [core] [Channel #5 SubChannel #6]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1012 23:13:41.282500       1 logging.go:55] [core] [Channel #58 SubChannel #59]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-apiserver [88f3c9aea1db] <==
I1018 18:10:09.856053       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1018 18:10:09.856218       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I1018 18:10:09.856235       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I1018 18:10:09.856280       1 controller.go:78] Starting OpenAPI AggregationController
I1018 18:10:09.856318       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I1018 18:10:09.856350       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I1018 18:10:09.873974       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I1018 18:10:09.874153       1 system_namespaces_controller.go:66] Starting system namespaces controller
I1018 18:10:09.874714       1 controller.go:142] Starting OpenAPI controller
I1018 18:10:09.875644       1 controller.go:90] Starting OpenAPI V3 controller
I1018 18:10:09.875764       1 naming_controller.go:294] Starting NamingConditionController
I1018 18:10:09.875888       1 establishing_controller.go:81] Starting EstablishingController
I1018 18:10:09.876007       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I1018 18:10:09.876126       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1018 18:10:09.876254       1 crd_finalizer.go:269] Starting CRDFinalizer
I1018 18:10:09.874071       1 local_available_controller.go:156] Starting LocalAvailability controller
I1018 18:10:09.876393       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I1018 18:10:09.876532       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1018 18:10:09.876641       1 cluster_authentication_trust_controller.go:443] Starting cluster_authentication_trust_controller controller
I1018 18:10:09.876706       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I1018 18:10:09.876762       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I1018 18:10:09.876771       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1018 18:10:09.877003       1 aggregator.go:169] waiting for initial CRD sync...
I1018 18:10:09.878669       1 controller.go:119] Starting legacy_token_tracking_controller
I1018 18:10:09.878686       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I1018 18:10:09.878737       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1018 18:10:09.878842       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1018 18:10:09.879020       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1018 18:10:09.962776       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I1018 18:10:09.962797       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I1018 18:10:09.976796       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1018 18:10:09.987668       1 handler_discovery.go:450] Starting ResourceDiscoveryManager
I1018 18:10:09.987870       1 shared_informer.go:320] Caches are synced for configmaps
I1018 18:10:09.995121       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1018 18:10:09.996253       1 cache.go:39] Caches are synced for LocalAvailability controller
I1018 18:10:10.028528       1 shared_informer.go:320] Caches are synced for node_authorizer
I1018 18:10:10.057323       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1018 18:10:10.057365       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1018 18:10:10.062908       1 shared_informer.go:320] Caches are synced for crd-autoregister
I1018 18:10:10.062997       1 aggregator.go:171] initial CRD sync complete...
I1018 18:10:10.063067       1 autoregister_controller.go:144] Starting autoregister controller
I1018 18:10:10.063095       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1018 18:10:10.063121       1 cache.go:39] Caches are synced for autoregister controller
I1018 18:10:10.073515       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I1018 18:10:10.073834       1 policy_source.go:224] refreshing policies
E1018 18:10:10.076436       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I1018 18:10:10.076897       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I1018 18:10:10.103888       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
E1018 18:10:10.236234       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: c6285762-63e3-49b8-aad2-a4ccf83150e7, UID in object meta: "
I1018 18:10:10.878265       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1018 18:10:12.148882       1 controller.go:615] quota admission added evaluator for: endpoints
I1018 18:10:29.912788       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1018 18:12:26.615557       1 controller.go:615] quota admission added evaluator for: namespaces
I1018 18:12:26.633527       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I1018 18:12:49.030655       1 controller.go:615] quota admission added evaluator for: deployments.apps
I1018 18:12:49.040115       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I1018 18:13:09.869935       1 alloc.go:330] "allocated clusterIPs" service="go-depi/go-app" clusterIPs={"IPv4":"10.111.156.228"}
I1018 18:13:31.690170       1 alloc.go:330] "allocated clusterIPs" service="go-depi/mongo-app-service" clusterIPs={"IPv4":"10.100.255.74"}
I1018 18:29:38.987888       1 alloc.go:330] "allocated clusterIPs" service="go-depi/go-app" clusterIPs={"IPv4":"10.97.244.25"}
I1018 18:29:39.202627       1 alloc.go:330] "allocated clusterIPs" service="go-depi/mongo-app-service" clusterIPs={"IPv4":"10.98.73.146"}


==> kube-controller-manager [46ce5f14e7c6] <==
I1012 12:47:14.828215       1 shared_informer.go:313] Waiting for caches to sync for garbage collector
I1012 12:47:14.828005       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I1012 12:47:14.828839       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I1012 12:47:14.829060       1 shared_informer.go:320] Caches are synced for GC
I1012 12:47:14.832130       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I1012 12:47:14.833009       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I1012 12:47:14.833564       1 range_allocator.go:422] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I1012 12:47:14.834889       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1012 12:47:14.835110       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1012 12:47:14.835881       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I1012 12:47:14.851698       1 shared_informer.go:320] Caches are synced for PV protection
I1012 12:47:14.857127       1 shared_informer.go:320] Caches are synced for crt configmap
I1012 12:47:14.864683       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I1012 12:47:14.865379       1 shared_informer.go:320] Caches are synced for endpoint
I1012 12:47:14.870138       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1012 12:47:14.870216       1 shared_informer.go:320] Caches are synced for attach detach
I1012 12:47:14.874703       1 shared_informer.go:320] Caches are synced for daemon sets
I1012 12:47:14.874753       1 shared_informer.go:320] Caches are synced for expand
I1012 12:47:14.876027       1 shared_informer.go:320] Caches are synced for endpoint_slice
I1012 12:47:14.877013       1 shared_informer.go:320] Caches are synced for service account
I1012 12:47:14.885932       1 shared_informer.go:320] Caches are synced for job
I1012 12:47:14.886369       1 shared_informer.go:320] Caches are synced for ephemeral
I1012 12:47:14.889950       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I1012 12:47:14.896282       1 shared_informer.go:320] Caches are synced for persistent volume
I1012 12:47:14.897696       1 shared_informer.go:320] Caches are synced for disruption
I1012 12:47:14.907274       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I1012 12:47:14.907369       1 shared_informer.go:320] Caches are synced for stateful set
I1012 12:47:14.911220       1 shared_informer.go:320] Caches are synced for namespace
I1012 12:47:14.915927       1 shared_informer.go:320] Caches are synced for PVC protection
I1012 12:47:14.918292       1 shared_informer.go:320] Caches are synced for HPA
I1012 12:47:14.919899       1 shared_informer.go:320] Caches are synced for taint
I1012 12:47:14.920124       1 node_lifecycle_controller.go:1232] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I1012 12:47:14.920482       1 node_lifecycle_controller.go:884] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I1012 12:47:14.920582       1 node_lifecycle_controller.go:1078] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I1012 12:47:14.928198       1 shared_informer.go:320] Caches are synced for deployment
I1012 12:47:14.955082       1 shared_informer.go:320] Caches are synced for ReplicaSet
I1012 12:47:14.984095       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1012 12:47:14.995389       1 shared_informer.go:320] Caches are synced for resource quota
I1012 12:47:14.997741       1 shared_informer.go:320] Caches are synced for resource quota
I1012 12:47:15.428592       1 shared_informer.go:320] Caches are synced for garbage collector
I1012 12:47:15.468506       1 shared_informer.go:320] Caches are synced for garbage collector
I1012 12:47:15.468554       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I1012 12:47:16.580710       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="526.013692ms"
I1012 12:47:16.713983       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="132.403626ms"
I1012 12:47:16.717500       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="207.14¬µs"
I1012 12:47:16.723496       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="157.513¬µs"
I1012 12:47:18.865373       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="1.421613ms"
I1012 12:47:20.570380       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1012 12:47:25.604004       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="21.195801ms"
I1012 12:47:25.604416       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="88.809¬µs"
I1012 12:52:28.267439       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1012 12:57:35.147832       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1012 13:02:40.135666       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1012 13:07:45.559390       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1012 13:12:50.860002       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1012 13:17:58.143916       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1012 13:23:04.705762       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1012 13:28:10.408586       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
E1012 23:13:12.649847       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
I1012 23:13:13.126973       1 garbagecollector.go:828] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"


==> kube-controller-manager [89cf1e185009] <==
I1018 18:10:13.855430       1 shared_informer.go:320] Caches are synced for resource quota
I1018 18:10:13.857289       1 shared_informer.go:320] Caches are synced for disruption
I1018 18:10:13.859996       1 shared_informer.go:320] Caches are synced for stateful set
I1018 18:10:13.882223       1 shared_informer.go:320] Caches are synced for resource quota
I1018 18:10:14.284900       1 shared_informer.go:320] Caches are synced for garbage collector
I1018 18:10:14.328771       1 shared_informer.go:320] Caches are synced for garbage collector
I1018 18:10:14.328988       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I1018 18:10:29.952442       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="144.351527ms"
I1018 18:10:29.953504       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="145.112¬µs"
I1018 18:10:40.361945       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="135.872¬µs"
I1018 18:10:49.805199       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="25.008944ms"
I1018 18:10:49.805403       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="101.168¬µs"
I1018 18:12:49.184149       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-844d77465d" duration="139.211182ms"
I1018 18:12:49.238137       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-844d77465d" duration="50.820385ms"
I1018 18:12:49.238392       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-844d77465d" duration="124.632¬µs"
I1018 18:12:49.241979       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-844d77465d" duration="103.302¬µs"
I1018 18:15:16.536365       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1018 18:20:22.288321       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1018 18:25:28.755265       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1018 18:26:43.599591       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-844d77465d" duration="170.781035ms"
I1018 18:26:44.032707       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-844d77465d" duration="432.855116ms"
I1018 18:26:44.104813       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-844d77465d" duration="71.90362ms"
I1018 18:26:44.113056       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-844d77465d" duration="161.192¬µs"
I1018 18:26:44.121996       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-844d77465d" duration="122.474¬µs"
I1018 18:26:44.569782       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-844d77465d" duration="30.105¬µs"
I1018 18:28:52.974249       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1018 18:29:39.176779       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-844d77465d" duration="271.349336ms"
I1018 18:29:39.310157       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-844d77465d" duration="133.234572ms"
I1018 18:29:39.310406       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-844d77465d" duration="128.599¬µs"
I1018 18:29:39.322555       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-844d77465d" duration="135.834¬µs"
I1018 18:29:56.870895       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-844d77465d" duration="105.775¬µs"
I1018 18:29:57.981971       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-844d77465d" duration="136.773¬µs"
I1018 18:29:58.072254       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-844d77465d" duration="137.966¬µs"
I1018 18:29:59.050493       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-844d77465d" duration="117.734¬µs"
I1018 18:30:25.798086       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-844d77465d" duration="142.452¬µs"
I1018 18:30:28.793349       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-844d77465d" duration="130.334¬µs"
I1018 18:30:39.794532       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-844d77465d" duration="151.678¬µs"
I1018 18:30:41.784908       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-844d77465d" duration="135.424¬µs"
I1018 18:30:52.780130       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-844d77465d" duration="200.737¬µs"
I1018 18:30:57.781182       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-844d77465d" duration="131.324¬µs"
I1018 18:31:04.779717       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-844d77465d" duration="182.032¬µs"
I1018 18:31:09.795179       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-844d77465d" duration="130.757¬µs"
I1018 18:31:45.776942       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-844d77465d" duration="128.792¬µs"
I1018 18:31:47.828942       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-844d77465d" duration="200.281¬µs"
I1018 18:31:57.118025       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-844d77465d" duration="111.984¬µs"
I1018 18:32:02.875973       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-844d77465d" duration="110.264¬µs"
I1018 18:33:24.787294       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-844d77465d" duration="178.029¬µs"
I1018 18:33:25.781583       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-844d77465d" duration="129.833¬µs"
I1018 18:33:36.775270       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-844d77465d" duration="113.004¬µs"
I1018 18:33:38.779392       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-844d77465d" duration="121.753¬µs"
I1018 18:33:59.065970       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1018 18:36:15.798419       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-844d77465d" duration="171.155¬µs"
I1018 18:36:18.778987       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-844d77465d" duration="158.694¬µs"
I1018 18:36:28.846341       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-844d77465d" duration="6.799359ms"
I1018 18:36:31.786002       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-844d77465d" duration="227.262¬µs"
I1018 18:37:54.380830       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-7b99dc756b" duration="96.37217ms"
I1018 18:37:54.441937       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-7b99dc756b" duration="60.974269ms"
I1018 18:37:54.442269       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-7b99dc756b" duration="144.88¬µs"
I1018 18:37:54.477541       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="go-depi/go-app-deploy-7b99dc756b" duration="119.461¬µs"
I1018 18:39:04.616436       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-proxy [889ad675a4ef] <==
I1012 12:47:17.790231       1 server_linux.go:66] "Using iptables proxy"
I1012 12:47:17.885308       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E1012 12:47:17.885370       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1012 12:47:17.906759       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1012 12:47:17.906813       1 server_linux.go:169] "Using iptables Proxier"
I1012 12:47:17.909758       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1012 12:47:17.910118       1 server.go:483] "Version info" version="v1.31.0"
I1012 12:47:17.910144       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1012 12:47:17.911493       1 config.go:326] "Starting node config controller"
I1012 12:47:17.911569       1 shared_informer.go:313] Waiting for caches to sync for node config
I1012 12:47:17.911755       1 config.go:197] "Starting service config controller"
I1012 12:47:17.911766       1 shared_informer.go:313] Waiting for caches to sync for service config
I1012 12:47:17.911783       1 config.go:104] "Starting endpoint slice config controller"
I1012 12:47:17.911791       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I1012 12:47:18.012484       1 shared_informer.go:320] Caches are synced for node config
I1012 12:47:18.012570       1 shared_informer.go:320] Caches are synced for service config
I1012 12:47:18.012615       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-proxy [e05a3782b997] <==
I1018 18:10:40.890265       1 server_linux.go:66] "Using iptables proxy"
I1018 18:10:42.798688       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E1018 18:10:42.798950       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1018 18:10:43.232815       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1018 18:10:43.232887       1 server_linux.go:169] "Using iptables Proxier"
I1018 18:10:43.237173       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1018 18:10:43.250813       1 server.go:483] "Version info" version="v1.31.0"
I1018 18:10:43.250870       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1018 18:10:43.366111       1 config.go:104] "Starting endpoint slice config controller"
I1018 18:10:43.366274       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I1018 18:10:43.366453       1 config.go:197] "Starting service config controller"
I1018 18:10:43.366482       1 shared_informer.go:313] Waiting for caches to sync for service config
I1018 18:10:43.366671       1 config.go:326] "Starting node config controller"
I1018 18:10:43.366698       1 shared_informer.go:313] Waiting for caches to sync for node config
I1018 18:10:43.466472       1 shared_informer.go:320] Caches are synced for endpoint slice config
I1018 18:10:43.466659       1 shared_informer.go:320] Caches are synced for service config
I1018 18:10:43.466861       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [619586b2bb15] <==
E1018 18:10:04.672895       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1018 18:10:04.673457       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1018 18:10:04.673648       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get \"https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1018 18:10:04.673656       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1018 18:10:04.673836       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1018 18:10:04.674669       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1018 18:10:04.674851       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1018 18:10:04.675489       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1018 18:10:04.675596       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get \"https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1018 18:10:04.675990       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1018 18:10:04.676175       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get \"https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1018 18:10:04.676138       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1018 18:10:04.676314       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get \"https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1018 18:10:04.678091       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1018 18:10:04.678242       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get \"https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1018 18:10:04.678682       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1018 18:10:04.678892       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get \"https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1018 18:10:04.679898       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1018 18:10:04.679951       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1018 18:10:04.679951       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1018 18:10:04.680135       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get \"https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
E1018 18:10:04.680133       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get \"https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
E1018 18:10:04.680143       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: Get \"https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1018 18:10:05.545553       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1018 18:10:05.545752       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1018 18:10:05.646914       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1018 18:10:05.647049       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get \"https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1018 18:10:05.662734       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1018 18:10:05.662847       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1018 18:10:09.911825       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1018 18:10:09.911860       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W1018 18:10:09.935115       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1018 18:10:09.935179       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1018 18:10:09.935115       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1018 18:10:09.935251       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1018 18:10:09.935372       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1018 18:10:09.935390       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1018 18:10:09.935372       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1018 18:10:09.935423       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1018 18:10:09.935453       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1018 18:10:09.935470       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W1018 18:10:09.935515       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1018 18:10:09.935532       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1018 18:10:09.935568       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1018 18:10:09.935588       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1018 18:10:09.935658       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1018 18:10:09.935676       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1018 18:10:09.935697       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1018 18:10:09.935725       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1018 18:10:09.935776       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W1018 18:10:09.935800       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1018 18:10:09.935810       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E1018 18:10:09.935816       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1018 18:10:09.935885       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1018 18:10:09.935903       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1018 18:10:09.935909       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1018 18:10:09.935928       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1018 18:10:09.936021       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1018 18:10:09.936042       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
I1018 18:10:12.574242       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kube-scheduler [6dee93b5030d] <==
W1012 12:47:07.054039       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1012 12:47:07.054401       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W1012 12:47:07.054546       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W1012 12:47:07.054570       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1012 12:47:07.054578       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
E1012 12:47:07.054598       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1012 12:47:07.054708       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1012 12:47:07.054735       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1012 12:47:07.054742       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W1012 12:47:07.054790       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W1012 12:47:07.054801       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1012 12:47:07.054810       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
E1012 12:47:07.054823       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E1012 12:47:07.054787       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1012 12:47:07.054904       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W1012 12:47:07.054922       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1012 12:47:07.054928       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
E1012 12:47:07.054937       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1012 12:47:07.054923       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1012 12:47:07.054991       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1012 12:47:07.054999       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W1012 12:47:07.055009       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1012 12:47:07.055014       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E1012 12:47:07.055030       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1012 12:47:07.055076       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1012 12:47:07.055094       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1012 12:47:07.055111       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1012 12:47:07.055136       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W1012 12:47:07.872236       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1012 12:47:07.872337       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1012 12:47:07.926823       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1012 12:47:07.926921       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1012 12:47:07.984832       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1012 12:47:07.984957       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1012 12:47:07.999986       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1012 12:47:08.000076       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1012 12:47:08.107535       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1012 12:47:08.107646       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1012 12:47:08.140378       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1012 12:47:08.140511       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W1012 12:47:08.145754       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1012 12:47:08.145859       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1012 12:47:08.168299       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1012 12:47:08.168386       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1012 12:47:08.228338       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1012 12:47:08.228651       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W1012 12:47:08.229059       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1012 12:47:08.229165       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1012 12:47:08.231027       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1012 12:47:08.231134       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1012 12:47:08.286466       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1012 12:47:08.287229       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1012 12:47:08.384851       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1012 12:47:08.385035       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1012 12:47:08.456389       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1012 12:47:08.456723       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1012 12:47:08.488423       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1012 12:47:08.488468       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
I1012 12:47:10.654648       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
E1012 23:13:31.896988       1 run.go:72] "command failed" err="finished without leader elect"


==> kubelet <==
Oct 18 18:33:48 minikube kubelet[1025]: E1018 18:33:48.751867    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-jsff4" podUID="b3fdf33c-eede-4603-88fa-30b631e7ab85"
Oct 18 18:33:53 minikube kubelet[1025]: E1018 18:33:53.753272    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-hvzzq" podUID="4320de62-1f37-4449-aef1-3162e8c5f9b6"
Oct 18 18:34:02 minikube kubelet[1025]: E1018 18:34:02.752423    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-jsff4" podUID="b3fdf33c-eede-4603-88fa-30b631e7ab85"
Oct 18 18:34:06 minikube kubelet[1025]: E1018 18:34:06.750828    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-hvzzq" podUID="4320de62-1f37-4449-aef1-3162e8c5f9b6"
Oct 18 18:34:16 minikube kubelet[1025]: E1018 18:34:16.750516    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-jsff4" podUID="b3fdf33c-eede-4603-88fa-30b631e7ab85"
Oct 18 18:34:21 minikube kubelet[1025]: E1018 18:34:21.751784    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-hvzzq" podUID="4320de62-1f37-4449-aef1-3162e8c5f9b6"
Oct 18 18:34:27 minikube kubelet[1025]: E1018 18:34:27.756772    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-jsff4" podUID="b3fdf33c-eede-4603-88fa-30b631e7ab85"
Oct 18 18:34:34 minikube kubelet[1025]: E1018 18:34:34.751688    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-hvzzq" podUID="4320de62-1f37-4449-aef1-3162e8c5f9b6"
Oct 18 18:34:38 minikube kubelet[1025]: E1018 18:34:38.751746    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-jsff4" podUID="b3fdf33c-eede-4603-88fa-30b631e7ab85"
Oct 18 18:34:47 minikube kubelet[1025]: E1018 18:34:47.761834    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-hvzzq" podUID="4320de62-1f37-4449-aef1-3162e8c5f9b6"
Oct 18 18:34:51 minikube kubelet[1025]: E1018 18:34:51.751643    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-jsff4" podUID="b3fdf33c-eede-4603-88fa-30b631e7ab85"
Oct 18 18:35:00 minikube kubelet[1025]: E1018 18:35:00.750596    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-hvzzq" podUID="4320de62-1f37-4449-aef1-3162e8c5f9b6"
Oct 18 18:35:05 minikube kubelet[1025]: E1018 18:35:05.750572    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-jsff4" podUID="b3fdf33c-eede-4603-88fa-30b631e7ab85"
Oct 18 18:35:14 minikube kubelet[1025]: E1018 18:35:14.751075    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-hvzzq" podUID="4320de62-1f37-4449-aef1-3162e8c5f9b6"
Oct 18 18:35:17 minikube kubelet[1025]: E1018 18:35:17.752581    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-jsff4" podUID="b3fdf33c-eede-4603-88fa-30b631e7ab85"
Oct 18 18:35:27 minikube kubelet[1025]: E1018 18:35:27.754387    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-hvzzq" podUID="4320de62-1f37-4449-aef1-3162e8c5f9b6"
Oct 18 18:35:28 minikube kubelet[1025]: E1018 18:35:28.756667    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-jsff4" podUID="b3fdf33c-eede-4603-88fa-30b631e7ab85"
Oct 18 18:35:39 minikube kubelet[1025]: E1018 18:35:39.755307    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-hvzzq" podUID="4320de62-1f37-4449-aef1-3162e8c5f9b6"
Oct 18 18:35:43 minikube kubelet[1025]: E1018 18:35:43.767539    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-jsff4" podUID="b3fdf33c-eede-4603-88fa-30b631e7ab85"
Oct 18 18:35:50 minikube kubelet[1025]: E1018 18:35:50.755933    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-hvzzq" podUID="4320de62-1f37-4449-aef1-3162e8c5f9b6"
Oct 18 18:36:01 minikube kubelet[1025]: E1018 18:36:01.039110    1025 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: manifest for omar421/depi-go:latest not found: manifest unknown: manifest unknown" image="omar421/depi-go:latest"
Oct 18 18:36:01 minikube kubelet[1025]: E1018 18:36:01.039266    1025 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: manifest for omar421/depi-go:latest not found: manifest unknown: manifest unknown" image="omar421/depi-go:latest"
Oct 18 18:36:01 minikube kubelet[1025]: E1018 18:36:01.039541    1025 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:go-app,Image:omar421/depi-go,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MONGODB_URI,Value:mongodb://mongo-app-service:27017,ValueFrom:nil,},EnvVar{Name:SERVER_PORT,Value:8080,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ss249,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod go-app-deploy-844d77465d-jsff4_go-depi(b3fdf33c-eede-4603-88fa-30b631e7ab85): ErrImagePull: Error response from daemon: manifest for omar421/depi-go:latest not found: manifest unknown: manifest unknown" logger="UnhandledError"
Oct 18 18:36:01 minikube kubelet[1025]: E1018 18:36:01.043289    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ErrImagePull: \"Error response from daemon: manifest for omar421/depi-go:latest not found: manifest unknown: manifest unknown\"" pod="go-depi/go-app-deploy-844d77465d-jsff4" podUID="b3fdf33c-eede-4603-88fa-30b631e7ab85"
Oct 18 18:36:05 minikube kubelet[1025]: E1018 18:36:05.273331    1025 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: manifest for omar421/depi-go:latest not found: manifest unknown: manifest unknown" image="omar421/depi-go:latest"
Oct 18 18:36:05 minikube kubelet[1025]: E1018 18:36:05.273430    1025 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: manifest for omar421/depi-go:latest not found: manifest unknown: manifest unknown" image="omar421/depi-go:latest"
Oct 18 18:36:05 minikube kubelet[1025]: E1018 18:36:05.273842    1025 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:go-app,Image:omar421/depi-go,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MONGODB_URI,Value:mongodb://mongo-app-service:27017,ValueFrom:nil,},EnvVar{Name:SERVER_PORT,Value:8080,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w64bd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod go-app-deploy-844d77465d-hvzzq_go-depi(4320de62-1f37-4449-aef1-3162e8c5f9b6): ErrImagePull: Error response from daemon: manifest for omar421/depi-go:latest not found: manifest unknown: manifest unknown" logger="UnhandledError"
Oct 18 18:36:05 minikube kubelet[1025]: E1018 18:36:05.275256    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ErrImagePull: \"Error response from daemon: manifest for omar421/depi-go:latest not found: manifest unknown: manifest unknown\"" pod="go-depi/go-app-deploy-844d77465d-hvzzq" podUID="4320de62-1f37-4449-aef1-3162e8c5f9b6"
Oct 18 18:36:15 minikube kubelet[1025]: E1018 18:36:15.767117    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-jsff4" podUID="b3fdf33c-eede-4603-88fa-30b631e7ab85"
Oct 18 18:36:18 minikube kubelet[1025]: E1018 18:36:18.753932    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-hvzzq" podUID="4320de62-1f37-4449-aef1-3162e8c5f9b6"
Oct 18 18:36:28 minikube kubelet[1025]: E1018 18:36:28.755142    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-jsff4" podUID="b3fdf33c-eede-4603-88fa-30b631e7ab85"
Oct 18 18:36:31 minikube kubelet[1025]: E1018 18:36:31.753224    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-hvzzq" podUID="4320de62-1f37-4449-aef1-3162e8c5f9b6"
Oct 18 18:36:40 minikube kubelet[1025]: E1018 18:36:40.750885    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-jsff4" podUID="b3fdf33c-eede-4603-88fa-30b631e7ab85"
Oct 18 18:36:44 minikube kubelet[1025]: E1018 18:36:44.750374    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-hvzzq" podUID="4320de62-1f37-4449-aef1-3162e8c5f9b6"
Oct 18 18:36:54 minikube kubelet[1025]: E1018 18:36:54.751014    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-jsff4" podUID="b3fdf33c-eede-4603-88fa-30b631e7ab85"
Oct 18 18:36:55 minikube kubelet[1025]: E1018 18:36:55.752325    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-hvzzq" podUID="4320de62-1f37-4449-aef1-3162e8c5f9b6"
Oct 18 18:37:06 minikube kubelet[1025]: E1018 18:37:06.751553    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-hvzzq" podUID="4320de62-1f37-4449-aef1-3162e8c5f9b6"
Oct 18 18:37:09 minikube kubelet[1025]: E1018 18:37:09.758696    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-jsff4" podUID="b3fdf33c-eede-4603-88fa-30b631e7ab85"
Oct 18 18:37:18 minikube kubelet[1025]: E1018 18:37:18.752310    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-hvzzq" podUID="4320de62-1f37-4449-aef1-3162e8c5f9b6"
Oct 18 18:37:21 minikube kubelet[1025]: E1018 18:37:21.753665    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-jsff4" podUID="b3fdf33c-eede-4603-88fa-30b631e7ab85"
Oct 18 18:37:31 minikube kubelet[1025]: E1018 18:37:31.750987    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-hvzzq" podUID="4320de62-1f37-4449-aef1-3162e8c5f9b6"
Oct 18 18:37:36 minikube kubelet[1025]: E1018 18:37:36.752268    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-jsff4" podUID="b3fdf33c-eede-4603-88fa-30b631e7ab85"
Oct 18 18:37:44 minikube kubelet[1025]: E1018 18:37:44.750849    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-hvzzq" podUID="4320de62-1f37-4449-aef1-3162e8c5f9b6"
Oct 18 18:37:47 minikube kubelet[1025]: E1018 18:37:47.753420    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-jsff4" podUID="b3fdf33c-eede-4603-88fa-30b631e7ab85"
Oct 18 18:37:54 minikube kubelet[1025]: I1018 18:37:54.436270    1025 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-zrpc8\" (UniqueName: \"kubernetes.io/projected/493361f9-336c-4fb1-b292-e6c2704f832d-kube-api-access-zrpc8\") pod \"go-app-deploy-7b99dc756b-4764g\" (UID: \"493361f9-336c-4fb1-b292-e6c2704f832d\") " pod="go-depi/go-app-deploy-7b99dc756b-4764g"
Oct 18 18:37:57 minikube kubelet[1025]: E1018 18:37:57.752113    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-hvzzq" podUID="4320de62-1f37-4449-aef1-3162e8c5f9b6"
Oct 18 18:38:02 minikube kubelet[1025]: E1018 18:38:02.753759    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-jsff4" podUID="b3fdf33c-eede-4603-88fa-30b631e7ab85"
Oct 18 18:38:12 minikube kubelet[1025]: E1018 18:38:12.769456    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-hvzzq" podUID="4320de62-1f37-4449-aef1-3162e8c5f9b6"
Oct 18 18:38:13 minikube kubelet[1025]: E1018 18:38:13.756636    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-jsff4" podUID="b3fdf33c-eede-4603-88fa-30b631e7ab85"
Oct 18 18:38:24 minikube kubelet[1025]: E1018 18:38:24.752053    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-jsff4" podUID="b3fdf33c-eede-4603-88fa-30b631e7ab85"
Oct 18 18:38:27 minikube kubelet[1025]: E1018 18:38:27.802397    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-hvzzq" podUID="4320de62-1f37-4449-aef1-3162e8c5f9b6"
Oct 18 18:38:38 minikube kubelet[1025]: E1018 18:38:38.751362    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-jsff4" podUID="b3fdf33c-eede-4603-88fa-30b631e7ab85"
Oct 18 18:38:42 minikube kubelet[1025]: E1018 18:38:42.762854    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-hvzzq" podUID="4320de62-1f37-4449-aef1-3162e8c5f9b6"
Oct 18 18:38:51 minikube kubelet[1025]: E1018 18:38:51.756241    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-jsff4" podUID="b3fdf33c-eede-4603-88fa-30b631e7ab85"
Oct 18 18:38:54 minikube kubelet[1025]: E1018 18:38:54.753030    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-hvzzq" podUID="4320de62-1f37-4449-aef1-3162e8c5f9b6"
Oct 18 18:39:02 minikube kubelet[1025]: E1018 18:39:02.757024    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-jsff4" podUID="b3fdf33c-eede-4603-88fa-30b631e7ab85"
Oct 18 18:39:09 minikube kubelet[1025]: E1018 18:39:09.751519    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-hvzzq" podUID="4320de62-1f37-4449-aef1-3162e8c5f9b6"
Oct 18 18:39:16 minikube kubelet[1025]: E1018 18:39:16.751278    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-jsff4" podUID="b3fdf33c-eede-4603-88fa-30b631e7ab85"
Oct 18 18:39:20 minikube kubelet[1025]: E1018 18:39:20.751228    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-hvzzq" podUID="4320de62-1f37-4449-aef1-3162e8c5f9b6"
Oct 18 18:39:31 minikube kubelet[1025]: E1018 18:39:31.756704    1025 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-app\" with ImagePullBackOff: \"Back-off pulling image \\\"omar421/depi-go\\\"\"" pod="go-depi/go-app-deploy-844d77465d-jsff4" podUID="b3fdf33c-eede-4603-88fa-30b631e7ab85"


==> storage-provisioner [731121783c98] <==
I1018 18:11:22.385226       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1018 18:11:22.463081       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1018 18:11:22.474492       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1018 18:11:39.964194       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1018 18:11:39.965465       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_55ff3752-99d6-4ea2-a835-35eca9c8a4e5!
I1018 18:11:39.965523       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"818a39f3-c6c5-49d0-a104-0f88e3c5d7de", APIVersion:"v1", ResourceVersion:"2581", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_55ff3752-99d6-4ea2-a835-35eca9c8a4e5 became leader
I1018 18:11:40.101327       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_55ff3752-99d6-4ea2-a835-35eca9c8a4e5!


==> storage-provisioner [b72b185387ab] <==
I1018 18:10:36.142191       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1018 18:11:06.202272       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

